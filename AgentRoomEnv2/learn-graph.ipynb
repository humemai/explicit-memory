{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6938, Acc: 0.4000\n",
      "Epoch 10, Loss: 0.6784, Acc: 0.6000\n",
      "Epoch 20, Loss: 0.6733, Acc: 0.6000\n",
      "Epoch 30, Loss: 0.6732, Acc: 0.6000\n",
      "Epoch 40, Loss: 0.6733, Acc: 0.6000\n",
      "Epoch 50, Loss: 0.6731, Acc: 0.6000\n",
      "Epoch 60, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 70, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 80, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 90, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 100, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 110, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 120, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 130, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 140, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 150, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 160, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 170, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 180, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 190, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 200, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 210, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 220, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 230, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 240, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 250, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 260, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 270, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 280, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 290, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 300, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 310, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 320, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 330, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 340, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 350, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 360, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 370, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 380, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 390, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 400, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 410, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 420, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 430, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 440, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 450, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 460, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 470, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 480, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 490, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 500, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 510, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 520, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 530, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 540, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 550, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 560, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 570, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 580, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 590, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 600, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 610, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 620, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 630, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 640, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 650, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 660, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 670, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 680, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 690, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 700, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 710, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 720, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 730, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 740, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 750, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 760, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 770, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 780, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 790, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 800, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 810, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 820, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 830, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 840, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 850, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 860, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 870, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 880, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 890, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 900, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 910, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 920, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 930, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 940, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 950, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 960, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 970, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 980, Loss: 0.6730, Acc: 0.6000\n",
      "Epoch 990, Loss: 0.6730, Acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Synthetic graph data\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 1],\n",
    "        [1, 0, 0],\n",
    "    ],\n",
    "    dtype=torch.float,\n",
    ")\n",
    "\n",
    "edge_index = torch.tensor(\n",
    "    [[0, 1, 2, 3, 0, 2, 3, 4], [1, 2, 3, 0, 2, 0, 4, 3]], dtype=torch.long\n",
    ")\n",
    "\n",
    "y = torch.tensor([0, 1, 0, 1, 1], dtype=torch.long)\n",
    "\n",
    "# Define a simple train mask\n",
    "# Here, we'll train on all nodes just for demonstration purposes\n",
    "train_mask = torch.tensor([True, True, True, True, True], dtype=torch.bool)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask)\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(3, 2)\n",
    "        self.conv2 = GCNConv(2, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        # print(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        _, predicted = torch.max(\n",
    "            out[data.train_mask], 1\n",
    "        )  # Get the index of the max log-probability\n",
    "        correct = (\n",
    "            (predicted == data.y[data.train_mask]).sum().item()\n",
    "        )  # Count how many predictions match the true labels\n",
    "        acc = correct / data.train_mask.sum().item()  # Calculate accuracy\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# Step 1: Generate a Synthetic Graph\n",
    "# Node features\n",
    "x = torch.tensor([[1, 0], [0, 1], [1, 1], [1, 0]], dtype=torch.float)\n",
    "# Edges\n",
    "edge_index = torch.tensor([[0, 1, 2, 3, 0, 2], \n",
    "                           [1, 0, 3, 2, 2, 0]], dtype=torch.long)\n",
    "# Edge labels for classification\n",
    "edge_label = torch.tensor([0, 1, 0, 1, 1, 0], dtype=torch.long)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Step 2: Define the GNN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN()\n",
    "\n",
    "# Edge classifier: A simple linear layer\n",
    "edge_classifier = torch.nn.Linear(16 * 2, 2)  # Assuming we concatenate two node embeddings\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(edge_classifier.parameters()), lr=0.01)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "for epoch in range(1000):\n",
    "    node_embeddings = model(data)\n",
    "    \n",
    "    # Get embeddings of both source and target nodes of each edge\n",
    "    source_embeddings = node_embeddings[data.edge_index[0]]\n",
    "    target_embeddings = node_embeddings[data.edge_index[1]]\n",
    "    \n",
    "    # Edge classification: Here we concatenate source and target embeddings\n",
    "    edge_embeddings = torch.cat([source_embeddings, target_embeddings], dim=1)\n",
    "    edge_pred = edge_classifier(edge_embeddings)\n",
    "    \n",
    "    loss = F.cross_entropy(edge_pred, edge_label)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    _, pred = torch.max(edge_pred, dim=1)\n",
    "    correct = pred.eq(edge_label).sum().item()\n",
    "    accuracy = correct / edge_label.size(0)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7016306519508362, Accuracy: 0.5\n",
      "Epoch 10, Loss: 0.6459193825721741, Accuracy: 0.8333333333333334\n",
      "Epoch 20, Loss: 0.42045748233795166, Accuracy: 0.6666666666666666\n",
      "Epoch 30, Loss: 0.08629082888364792, Accuracy: 1.0\n",
      "Epoch 40, Loss: 0.0052810912020504475, Accuracy: 1.0\n",
      "Epoch 50, Loss: 0.00060127186588943, Accuracy: 1.0\n",
      "Epoch 60, Loss: 0.00018166076915804297, Accuracy: 1.0\n",
      "Epoch 70, Loss: 0.00010940333595499396, Accuracy: 1.0\n",
      "Epoch 80, Loss: 8.832736784825101e-05, Accuracy: 1.0\n",
      "Epoch 90, Loss: 7.98052569734864e-05, Accuracy: 1.0\n",
      "Epoch 100, Loss: 7.515673496527597e-05, Accuracy: 1.0\n",
      "Epoch 110, Loss: 7.18192895874381e-05, Accuracy: 1.0\n",
      "Epoch 120, Loss: 6.905792542966083e-05, Accuracy: 1.0\n",
      "Epoch 130, Loss: 6.649520219070837e-05, Accuracy: 1.0\n",
      "Epoch 140, Loss: 6.40715443296358e-05, Accuracy: 1.0\n",
      "Epoch 150, Loss: 6.174719601403922e-05, Accuracy: 1.0\n",
      "Epoch 160, Loss: 5.948244870523922e-05, Accuracy: 1.0\n",
      "Epoch 170, Loss: 5.731702549383044e-05, Accuracy: 1.0\n",
      "Epoch 180, Loss: 5.5310516472673044e-05, Accuracy: 1.0\n",
      "Epoch 190, Loss: 5.344307282939553e-05, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Synthetic Graph Data Setup (including node features, edges, and edge labels)\n",
    "x = torch.tensor([[1, 0], [0, 1], [1, 1], [1, 0]], dtype=torch.float)  # Node features\n",
    "edge_index = torch.tensor(\n",
    "    [[0, 1, 2, 3, 0, 2], [1, 0, 3, 2, 2, 0]], dtype=torch.long\n",
    ")  # Edges\n",
    "edge_label = torch.tensor(\n",
    "    [0, 1, 0, 1, 1, 0], dtype=torch.long\n",
    ")  # Edge labels for classification\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "# EnhancedGCN Model Definition with Learnable Embeddings\n",
    "class EnhancedGCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_node_features, num_embedding_features):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(num_nodes, num_embedding_features)\n",
    "        self.embeddings.weight.data.uniform_(0, 1)  # Initialize embeddings\n",
    "\n",
    "        self.conv1 = GCNConv(num_embedding_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        node_indices = torch.arange(0, x.size(0), dtype=torch.long, device=x.device)\n",
    "        node_embeddings = self.embeddings(node_indices)\n",
    "\n",
    "        # x = torch.cat([x, node_embeddings], dim=1)  # Concatenate original features with embeddings\n",
    "        # x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv1(node_embeddings, edge_index))\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model and Optimizer Setup\n",
    "num_nodes = data.num_nodes\n",
    "num_node_features = data.num_features\n",
    "num_embedding_features = 5  # Example additional embedding size\n",
    "model = EnhancedGCN(num_nodes, num_node_features, num_embedding_features)\n",
    "\n",
    "edge_classifier = torch.nn.Linear(\n",
    "    16 * 2, 2\n",
    ")  # Classifier for the concatenated node embeddings\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(edge_classifier.parameters()), lr=0.01\n",
    ")\n",
    "\n",
    "# Training Loop with Accuracy Calculation for Edge Classification\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    node_embeddings = model(data)\n",
    "\n",
    "    source_embeddings = node_embeddings[data.edge_index[0]]\n",
    "    target_embeddings = node_embeddings[data.edge_index[1]]\n",
    "    edge_embeddings = torch.cat([source_embeddings, target_embeddings], dim=1)\n",
    "\n",
    "    edge_pred = edge_classifier(edge_embeddings)\n",
    "    loss = F.cross_entropy(edge_pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    _, pred = torch.max(edge_pred, dim=1)\n",
    "    correct = pred.eq(edge_label).sum().item()\n",
    "    accuracy = correct / edge_label.size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7564857602119446, Accuracy: 0.5\n",
      "Epoch 10, Loss: 0.6708261370658875, Accuracy: 0.5\n",
      "Epoch 20, Loss: 0.5375721454620361, Accuracy: 1.0\n",
      "Epoch 30, Loss: 0.2010474056005478, Accuracy: 1.0\n",
      "Epoch 40, Loss: 0.02258077822625637, Accuracy: 1.0\n",
      "Epoch 50, Loss: 0.0017681272001937032, Accuracy: 1.0\n",
      "Epoch 60, Loss: 0.0004274191742297262, Accuracy: 1.0\n",
      "Epoch 70, Loss: 0.00013457309978548437, Accuracy: 1.0\n",
      "Epoch 80, Loss: 4.569508018903434e-05, Accuracy: 1.0\n",
      "Epoch 90, Loss: 1.9251956473453902e-05, Accuracy: 1.0\n",
      "Epoch 100, Loss: 1.0351240234740544e-05, Accuracy: 1.0\n",
      "Epoch 110, Loss: 6.7551518441177905e-06, Accuracy: 1.0\n",
      "Epoch 120, Loss: 5.026635790272849e-06, Accuracy: 1.0\n",
      "Epoch 130, Loss: 3.993497557530645e-06, Accuracy: 1.0\n",
      "Epoch 140, Loss: 3.3179824185936013e-06, Accuracy: 1.0\n",
      "Epoch 150, Loss: 2.841147761500906e-06, Accuracy: 1.0\n",
      "Epoch 160, Loss: 2.463653572704061e-06, Accuracy: 1.0\n",
      "Epoch 170, Loss: 2.1656312583218096e-06, Accuracy: 1.0\n",
      "Epoch 180, Loss: 1.9272138160886243e-06, Accuracy: 1.0\n",
      "Epoch 190, Loss: 1.7086640582419932e-06, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class EnhancedGCNWithEdgeClassification(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_node_features, num_embedding_features, out_channels):\n",
    "        super(EnhancedGCNWithEdgeClassification, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(num_nodes, num_embedding_features)\n",
    "        self.embeddings.weight.data.uniform_(0, 1)  # Initialize embeddings\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features + num_embedding_features, 16)\n",
    "        self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        # Instead of a separate edge classifier, incorporate it into the GNN model\n",
    "        self.edge_classifier = torch.nn.Linear(out_channels * 2, 2)  # Assuming binary classification for edges\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        node_indices = torch.arange(0, x.size(0), dtype=torch.long, device=x.device)\n",
    "        node_embeddings = self.embeddings(node_indices)\n",
    "        x = torch.cat([x, node_embeddings], dim=1)  # Concatenate original features with embeddings\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Now, for each edge, use the node embeddings to predict edge labels\n",
    "        source_embeddings = x[edge_index[0]]\n",
    "        target_embeddings = x[edge_index[1]]\n",
    "        edge_embeddings = torch.cat([source_embeddings, target_embeddings], dim=1)\n",
    "        edge_pred = self.edge_classifier(edge_embeddings)\n",
    "\n",
    "        return edge_pred\n",
    "\n",
    "# Parameters and Data Initialization\n",
    "num_nodes = data.num_nodes\n",
    "num_node_features = data.num_features\n",
    "num_embedding_features = 5  # Example additional embedding size\n",
    "out_channels = 16  # Example size for the node embeddings used in edge predictions\n",
    "\n",
    "model = EnhancedGCNWithEdgeClassification(num_nodes, num_node_features, num_embedding_features, out_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Assume edge_index, x, and edge_label are defined as in the previous examples\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Training Loop with the Adjusted Model\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    edge_pred = model(data)  # The model now directly outputs edge predictions\n",
    "    loss = F.cross_entropy(edge_pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    _, pred = torch.max(edge_pred, dim=1)\n",
    "    correct = pred.eq(edge_label).sum().item()\n",
    "    accuracy = correct / edge_label.size(0)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -7.742973327636719\n",
      "Epoch 1, Loss: -7.680062294006348\n",
      "Epoch 2, Loss: -7.813106536865234\n",
      "Epoch 3, Loss: -7.8224077224731445\n",
      "Epoch 4, Loss: -7.743018627166748\n",
      "Epoch 5, Loss: -8.202510833740234\n",
      "Epoch 6, Loss: -7.553354740142822\n",
      "Epoch 7, Loss: -8.010868072509766\n",
      "Epoch 8, Loss: -7.105863571166992\n",
      "Epoch 9, Loss: -8.162364959716797\n",
      "Epoch 10, Loss: -7.905317783355713\n",
      "Epoch 11, Loss: -7.111297130584717\n",
      "Epoch 12, Loss: -7.412032127380371\n",
      "Epoch 13, Loss: -7.288893222808838\n",
      "Epoch 14, Loss: -7.157147407531738\n",
      "Epoch 15, Loss: -7.519998073577881\n",
      "Epoch 16, Loss: -7.834115505218506\n",
      "Epoch 17, Loss: -8.216315269470215\n",
      "Epoch 18, Loss: -7.244935512542725\n",
      "Epoch 19, Loss: -7.591847896575928\n",
      "Epoch 20, Loss: -7.370690822601318\n",
      "Epoch 21, Loss: -7.748913288116455\n",
      "Epoch 22, Loss: -7.396695137023926\n",
      "Epoch 23, Loss: -8.078722953796387\n",
      "Epoch 24, Loss: -7.408994674682617\n",
      "Epoch 25, Loss: -7.649755477905273\n",
      "Epoch 26, Loss: -7.609882831573486\n",
      "Epoch 27, Loss: -7.472285270690918\n",
      "Epoch 28, Loss: -7.769733428955078\n",
      "Epoch 29, Loss: -7.901911735534668\n",
      "Epoch 30, Loss: -7.156304836273193\n",
      "Epoch 31, Loss: -7.537949085235596\n",
      "Epoch 32, Loss: -7.1531548500061035\n",
      "Epoch 33, Loss: -7.365673542022705\n",
      "Epoch 34, Loss: -7.215046405792236\n",
      "Epoch 35, Loss: -6.8748297691345215\n",
      "Epoch 36, Loss: -7.534071922302246\n",
      "Epoch 37, Loss: -6.971118450164795\n",
      "Epoch 38, Loss: -7.179816246032715\n",
      "Epoch 39, Loss: -7.8539628982543945\n",
      "Epoch 40, Loss: -7.531057834625244\n",
      "Epoch 41, Loss: -7.103774070739746\n",
      "Epoch 42, Loss: -7.302676677703857\n",
      "Epoch 43, Loss: -7.202498435974121\n",
      "Epoch 44, Loss: -7.7534332275390625\n",
      "Epoch 45, Loss: -7.691716194152832\n",
      "Epoch 46, Loss: -7.624504089355469\n",
      "Epoch 47, Loss: -7.299200057983398\n",
      "Epoch 48, Loss: -7.185403823852539\n",
      "Epoch 49, Loss: -6.940850734710693\n",
      "Epoch 50, Loss: -7.512860298156738\n",
      "Epoch 51, Loss: -7.157609462738037\n",
      "Epoch 52, Loss: -6.897226810455322\n",
      "Epoch 53, Loss: -7.3367509841918945\n",
      "Epoch 54, Loss: -7.080990314483643\n",
      "Epoch 55, Loss: -7.133580207824707\n",
      "Epoch 56, Loss: -7.406336784362793\n",
      "Epoch 57, Loss: -6.71673583984375\n",
      "Epoch 58, Loss: -6.720027446746826\n",
      "Epoch 59, Loss: -6.438836097717285\n",
      "Epoch 60, Loss: -7.2160468101501465\n",
      "Epoch 61, Loss: -7.050616264343262\n",
      "Epoch 62, Loss: -7.25382137298584\n",
      "Epoch 63, Loss: -6.925450325012207\n",
      "Epoch 64, Loss: -7.0778985023498535\n",
      "Epoch 65, Loss: -6.784339904785156\n",
      "Epoch 66, Loss: -6.296026706695557\n",
      "Epoch 67, Loss: -7.239380836486816\n",
      "Epoch 68, Loss: -7.294602394104004\n",
      "Epoch 69, Loss: -7.1991868019104\n",
      "Epoch 70, Loss: -7.544665336608887\n",
      "Epoch 71, Loss: -6.836098670959473\n",
      "Epoch 72, Loss: -7.001278400421143\n",
      "Epoch 73, Loss: -6.472702980041504\n",
      "Epoch 74, Loss: -6.912676811218262\n",
      "Epoch 75, Loss: -7.029498100280762\n",
      "Epoch 76, Loss: -6.908527374267578\n",
      "Epoch 77, Loss: -7.032842636108398\n",
      "Epoch 78, Loss: -6.615264415740967\n",
      "Epoch 79, Loss: -7.01414155960083\n",
      "Epoch 80, Loss: -6.761105537414551\n",
      "Epoch 81, Loss: -6.5970964431762695\n",
      "Epoch 82, Loss: -6.920534133911133\n",
      "Epoch 83, Loss: -6.979144096374512\n",
      "Epoch 84, Loss: -6.628958702087402\n",
      "Epoch 85, Loss: -6.844524383544922\n",
      "Epoch 86, Loss: -7.014463424682617\n",
      "Epoch 87, Loss: -6.8155436515808105\n",
      "Epoch 88, Loss: -6.274809837341309\n",
      "Epoch 89, Loss: -6.802736282348633\n",
      "Epoch 90, Loss: -6.7133893966674805\n",
      "Epoch 91, Loss: -6.482316017150879\n",
      "Epoch 92, Loss: -6.706389904022217\n",
      "Epoch 93, Loss: -6.46449613571167\n",
      "Epoch 94, Loss: -6.548506736755371\n",
      "Epoch 95, Loss: -6.810980319976807\n",
      "Epoch 96, Loss: -6.686854362487793\n",
      "Epoch 97, Loss: -6.493123531341553\n",
      "Epoch 98, Loss: -6.589921951293945\n",
      "Epoch 99, Loss: -7.262610912322998\n",
      "Epoch 100, Loss: -5.769769191741943\n",
      "Epoch 101, Loss: -7.203604698181152\n",
      "Epoch 102, Loss: -6.5424699783325195\n",
      "Epoch 103, Loss: -6.477278709411621\n",
      "Epoch 104, Loss: -6.27017068862915\n",
      "Epoch 105, Loss: -6.531923770904541\n",
      "Epoch 106, Loss: -6.598543167114258\n",
      "Epoch 107, Loss: -6.786084175109863\n",
      "Epoch 108, Loss: -6.741419315338135\n",
      "Epoch 109, Loss: -6.0063629150390625\n",
      "Epoch 110, Loss: -6.604098320007324\n",
      "Epoch 111, Loss: -6.008785247802734\n",
      "Epoch 112, Loss: -6.352872848510742\n",
      "Epoch 113, Loss: -6.39543342590332\n",
      "Epoch 114, Loss: -6.701353549957275\n",
      "Epoch 115, Loss: -6.424685478210449\n",
      "Epoch 116, Loss: -6.190312385559082\n",
      "Epoch 117, Loss: -6.131669521331787\n",
      "Epoch 118, Loss: -6.0375518798828125\n",
      "Epoch 119, Loss: -6.023972034454346\n",
      "Epoch 120, Loss: -5.66478967666626\n",
      "Epoch 121, Loss: -6.339178085327148\n",
      "Epoch 122, Loss: -6.312556266784668\n",
      "Epoch 123, Loss: -5.659755229949951\n",
      "Epoch 124, Loss: -6.1858954429626465\n",
      "Epoch 125, Loss: -7.009976863861084\n",
      "Epoch 126, Loss: -5.759288787841797\n",
      "Epoch 127, Loss: -6.323179244995117\n",
      "Epoch 128, Loss: -5.780800819396973\n",
      "Epoch 129, Loss: -5.822617530822754\n",
      "Epoch 130, Loss: -5.679484844207764\n",
      "Epoch 131, Loss: -5.517055511474609\n",
      "Epoch 132, Loss: -5.522689342498779\n",
      "Epoch 133, Loss: -6.243618011474609\n",
      "Epoch 134, Loss: -5.806011199951172\n",
      "Epoch 135, Loss: -5.942624568939209\n",
      "Epoch 136, Loss: -6.336615085601807\n",
      "Epoch 137, Loss: -5.762424468994141\n",
      "Epoch 138, Loss: -5.840810298919678\n",
      "Epoch 139, Loss: -5.783056259155273\n",
      "Epoch 140, Loss: -5.6719136238098145\n",
      "Epoch 141, Loss: -5.798946857452393\n",
      "Epoch 142, Loss: -5.637406349182129\n",
      "Epoch 143, Loss: -6.11299991607666\n",
      "Epoch 144, Loss: -5.53279972076416\n",
      "Epoch 145, Loss: -6.311362266540527\n",
      "Epoch 146, Loss: -5.9792046546936035\n",
      "Epoch 147, Loss: -5.9610981941223145\n",
      "Epoch 148, Loss: -5.68293571472168\n",
      "Epoch 149, Loss: -5.645452499389648\n",
      "Epoch 150, Loss: -5.729876518249512\n",
      "Epoch 151, Loss: -5.483315944671631\n",
      "Epoch 152, Loss: -5.470064163208008\n",
      "Epoch 153, Loss: -5.532284259796143\n",
      "Epoch 154, Loss: -5.273942947387695\n",
      "Epoch 155, Loss: -5.334791660308838\n",
      "Epoch 156, Loss: -6.043630123138428\n",
      "Epoch 157, Loss: -4.89894437789917\n",
      "Epoch 158, Loss: -5.820525646209717\n",
      "Epoch 159, Loss: -5.438302040100098\n",
      "Epoch 160, Loss: -5.682138442993164\n",
      "Epoch 161, Loss: -5.787511348724365\n",
      "Epoch 162, Loss: -5.519470691680908\n",
      "Epoch 163, Loss: -4.713095664978027\n",
      "Epoch 164, Loss: -5.971782684326172\n",
      "Epoch 165, Loss: -5.477962017059326\n",
      "Epoch 166, Loss: -5.000701904296875\n",
      "Epoch 167, Loss: -5.741398811340332\n",
      "Epoch 168, Loss: -5.239965915679932\n",
      "Epoch 169, Loss: -5.693813800811768\n",
      "Epoch 170, Loss: -5.381564140319824\n",
      "Epoch 171, Loss: -5.637574672698975\n",
      "Epoch 172, Loss: -5.0807929039001465\n",
      "Epoch 173, Loss: -5.373555660247803\n",
      "Epoch 174, Loss: -5.046562194824219\n",
      "Epoch 175, Loss: -5.682003498077393\n",
      "Epoch 176, Loss: -5.3481125831604\n",
      "Epoch 177, Loss: -4.825918674468994\n",
      "Epoch 178, Loss: -5.0473809242248535\n",
      "Epoch 179, Loss: -4.2640581130981445\n",
      "Epoch 180, Loss: -5.245913505554199\n",
      "Epoch 181, Loss: -4.95863151550293\n",
      "Epoch 182, Loss: -4.936344146728516\n",
      "Epoch 183, Loss: -4.98270320892334\n",
      "Epoch 184, Loss: -4.5337419509887695\n",
      "Epoch 185, Loss: -4.958657741546631\n",
      "Epoch 186, Loss: -5.086697578430176\n",
      "Epoch 187, Loss: -4.782310962677002\n",
      "Epoch 188, Loss: -5.405375957489014\n",
      "Epoch 189, Loss: -4.898563385009766\n",
      "Epoch 190, Loss: -5.043748378753662\n",
      "Epoch 191, Loss: -4.351369857788086\n",
      "Epoch 192, Loss: -5.221548080444336\n",
      "Epoch 193, Loss: -5.071567535400391\n",
      "Epoch 194, Loss: -4.511063575744629\n",
      "Epoch 195, Loss: -4.747114181518555\n",
      "Epoch 196, Loss: -4.659502983093262\n",
      "Epoch 197, Loss: -4.885140419006348\n",
      "Epoch 198, Loss: -4.667304992675781\n",
      "Epoch 199, Loss: -4.645142078399658\n",
      "Epoch 200, Loss: -4.750586986541748\n",
      "Epoch 201, Loss: -5.149101257324219\n",
      "Epoch 202, Loss: -4.324589729309082\n",
      "Epoch 203, Loss: -4.378162860870361\n",
      "Epoch 204, Loss: -5.083069324493408\n",
      "Epoch 205, Loss: -4.252825736999512\n",
      "Epoch 206, Loss: -5.065377235412598\n",
      "Epoch 207, Loss: -5.396033763885498\n",
      "Epoch 208, Loss: -3.896573781967163\n",
      "Epoch 209, Loss: -4.714521408081055\n",
      "Epoch 210, Loss: -4.249679088592529\n",
      "Epoch 211, Loss: -4.999024868011475\n",
      "Epoch 212, Loss: -4.333959579467773\n",
      "Epoch 213, Loss: -4.462892532348633\n",
      "Epoch 214, Loss: -4.524807453155518\n",
      "Epoch 215, Loss: -4.881594657897949\n",
      "Epoch 216, Loss: -4.589057445526123\n",
      "Epoch 217, Loss: -4.2467803955078125\n",
      "Epoch 218, Loss: -4.330737113952637\n",
      "Epoch 219, Loss: -3.9991447925567627\n",
      "Epoch 220, Loss: -4.601106643676758\n",
      "Epoch 221, Loss: -3.8788416385650635\n",
      "Epoch 222, Loss: -4.204153537750244\n",
      "Epoch 223, Loss: -3.8329503536224365\n",
      "Epoch 224, Loss: -4.310824394226074\n",
      "Epoch 225, Loss: -3.6943612098693848\n",
      "Epoch 226, Loss: -4.049969673156738\n",
      "Epoch 227, Loss: -4.092465877532959\n",
      "Epoch 228, Loss: -3.98799991607666\n",
      "Epoch 229, Loss: -4.037148952484131\n",
      "Epoch 230, Loss: -4.370362281799316\n",
      "Epoch 231, Loss: -4.355512619018555\n",
      "Epoch 232, Loss: -4.105466365814209\n",
      "Epoch 233, Loss: -3.7499260902404785\n",
      "Epoch 234, Loss: -3.60827898979187\n",
      "Epoch 235, Loss: -4.028979301452637\n",
      "Epoch 236, Loss: -3.6669816970825195\n",
      "Epoch 237, Loss: -3.7647852897644043\n",
      "Epoch 238, Loss: -3.75938081741333\n",
      "Epoch 239, Loss: -3.7816734313964844\n",
      "Epoch 240, Loss: -4.016806602478027\n",
      "Epoch 241, Loss: -3.6187448501586914\n",
      "Epoch 242, Loss: -3.6800265312194824\n",
      "Epoch 243, Loss: -3.755857467651367\n",
      "Epoch 244, Loss: -3.9251468181610107\n",
      "Epoch 245, Loss: -3.472487449645996\n",
      "Epoch 246, Loss: -3.365023374557495\n",
      "Epoch 247, Loss: -3.259828805923462\n",
      "Epoch 248, Loss: -3.626984119415283\n",
      "Epoch 249, Loss: -4.005743980407715\n",
      "Epoch 250, Loss: -3.4176862239837646\n",
      "Epoch 251, Loss: -2.492014169692993\n",
      "Epoch 252, Loss: -3.18989634513855\n",
      "Epoch 253, Loss: -3.513371467590332\n",
      "Epoch 254, Loss: -4.144147872924805\n",
      "Epoch 255, Loss: -3.691070556640625\n",
      "Epoch 256, Loss: -3.0424675941467285\n",
      "Epoch 257, Loss: -3.7162506580352783\n",
      "Epoch 258, Loss: -3.1843388080596924\n",
      "Epoch 259, Loss: -2.638047933578491\n",
      "Epoch 260, Loss: -3.5998289585113525\n",
      "Epoch 261, Loss: -2.803983211517334\n",
      "Epoch 262, Loss: -4.088169097900391\n",
      "Epoch 263, Loss: -3.4317517280578613\n",
      "Epoch 264, Loss: -3.8210721015930176\n",
      "Epoch 265, Loss: -3.6417417526245117\n",
      "Epoch 266, Loss: -3.5454602241516113\n",
      "Epoch 267, Loss: -3.1075448989868164\n",
      "Epoch 268, Loss: -3.089751720428467\n",
      "Epoch 269, Loss: -3.7131757736206055\n",
      "Epoch 270, Loss: -3.264967679977417\n",
      "Epoch 271, Loss: -3.75939679145813\n",
      "Epoch 272, Loss: -3.2778661251068115\n",
      "Epoch 273, Loss: -3.059882164001465\n",
      "Epoch 274, Loss: -2.7667624950408936\n",
      "Epoch 275, Loss: -2.6517856121063232\n",
      "Epoch 276, Loss: -2.836699962615967\n",
      "Epoch 277, Loss: -3.0394163131713867\n",
      "Epoch 278, Loss: -2.6698780059814453\n",
      "Epoch 279, Loss: -3.4001269340515137\n",
      "Epoch 280, Loss: -3.097851276397705\n",
      "Epoch 281, Loss: -2.5173099040985107\n",
      "Epoch 282, Loss: -2.669553756713867\n",
      "Epoch 283, Loss: -3.526226043701172\n",
      "Epoch 284, Loss: -3.207054376602173\n",
      "Epoch 285, Loss: -3.2666850090026855\n",
      "Epoch 286, Loss: -3.4172441959381104\n",
      "Epoch 287, Loss: -3.113588333129883\n",
      "Epoch 288, Loss: -2.600125789642334\n",
      "Epoch 289, Loss: -3.206317901611328\n",
      "Epoch 290, Loss: -3.0334434509277344\n",
      "Epoch 291, Loss: -3.2108144760131836\n",
      "Epoch 292, Loss: -2.787569046020508\n",
      "Epoch 293, Loss: -3.189366579055786\n",
      "Epoch 294, Loss: -3.2981057167053223\n",
      "Epoch 295, Loss: -3.1810801029205322\n",
      "Epoch 296, Loss: -2.5116724967956543\n",
      "Epoch 297, Loss: -3.3740153312683105\n",
      "Epoch 298, Loss: -2.8255395889282227\n",
      "Epoch 299, Loss: -2.873944044113159\n",
      "Epoch 300, Loss: -2.72174072265625\n",
      "Epoch 301, Loss: -2.3557791709899902\n",
      "Epoch 302, Loss: -2.4161834716796875\n",
      "Epoch 303, Loss: -2.6591978073120117\n",
      "Epoch 304, Loss: -2.92646861076355\n",
      "Epoch 305, Loss: -2.570192337036133\n",
      "Epoch 306, Loss: -2.26039981842041\n",
      "Epoch 307, Loss: -2.8369429111480713\n",
      "Epoch 308, Loss: -2.1248886585235596\n",
      "Epoch 309, Loss: -2.7164156436920166\n",
      "Epoch 310, Loss: -3.097752332687378\n",
      "Epoch 311, Loss: -1.440101981163025\n",
      "Epoch 312, Loss: -2.9090476036071777\n",
      "Epoch 313, Loss: -2.767737627029419\n",
      "Epoch 314, Loss: -3.0423920154571533\n",
      "Epoch 315, Loss: -2.226839065551758\n",
      "Epoch 316, Loss: -2.095813274383545\n",
      "Epoch 317, Loss: -2.291487216949463\n",
      "Epoch 318, Loss: -2.5734047889709473\n",
      "Epoch 319, Loss: -2.1734750270843506\n",
      "Epoch 320, Loss: -2.4258406162261963\n",
      "Epoch 321, Loss: -2.309406042098999\n",
      "Epoch 322, Loss: -2.579927444458008\n",
      "Epoch 323, Loss: -2.2809219360351562\n",
      "Epoch 324, Loss: -2.6724562644958496\n",
      "Epoch 325, Loss: -2.593019962310791\n",
      "Epoch 326, Loss: -2.2472939491271973\n",
      "Epoch 327, Loss: -2.4273977279663086\n",
      "Epoch 328, Loss: -2.667393207550049\n",
      "Epoch 329, Loss: -2.718330144882202\n",
      "Epoch 330, Loss: -2.2645680904388428\n",
      "Epoch 331, Loss: -2.6979458332061768\n",
      "Epoch 332, Loss: -2.8205273151397705\n",
      "Epoch 333, Loss: -1.9479596614837646\n",
      "Epoch 334, Loss: -2.420680284500122\n",
      "Epoch 335, Loss: -2.146177291870117\n",
      "Epoch 336, Loss: -2.7258949279785156\n",
      "Epoch 337, Loss: -2.7547974586486816\n",
      "Epoch 338, Loss: -1.92744779586792\n",
      "Epoch 339, Loss: -2.3911285400390625\n",
      "Epoch 340, Loss: -2.2666049003601074\n",
      "Epoch 341, Loss: -2.2441086769104004\n",
      "Epoch 342, Loss: -2.371751546859741\n",
      "Epoch 343, Loss: -1.814414143562317\n",
      "Epoch 344, Loss: -2.420276165008545\n",
      "Epoch 345, Loss: -1.879414677619934\n",
      "Epoch 346, Loss: -1.8881416320800781\n",
      "Epoch 347, Loss: -1.7762770652770996\n",
      "Epoch 348, Loss: -1.5826168060302734\n",
      "Epoch 349, Loss: -3.4404776096343994\n",
      "Epoch 350, Loss: -1.8099415302276611\n",
      "Epoch 351, Loss: -1.8284411430358887\n",
      "Epoch 352, Loss: -1.6922693252563477\n",
      "Epoch 353, Loss: -1.6398303508758545\n",
      "Epoch 354, Loss: -1.51090407371521\n",
      "Epoch 355, Loss: -1.6870877742767334\n",
      "Epoch 356, Loss: -1.362095832824707\n",
      "Epoch 357, Loss: -2.1804251670837402\n",
      "Epoch 358, Loss: -1.6504772901535034\n",
      "Epoch 359, Loss: -1.9313498735427856\n",
      "Epoch 360, Loss: -1.666489601135254\n",
      "Epoch 361, Loss: -1.9092345237731934\n",
      "Epoch 362, Loss: -2.0991437435150146\n",
      "Epoch 363, Loss: -1.7148799896240234\n",
      "Epoch 364, Loss: -1.6707013845443726\n",
      "Epoch 365, Loss: -1.57049560546875\n",
      "Epoch 366, Loss: -1.7201833724975586\n",
      "Epoch 367, Loss: -1.410898208618164\n",
      "Epoch 368, Loss: -2.3576810359954834\n",
      "Epoch 369, Loss: -2.1398894786834717\n",
      "Epoch 370, Loss: -1.8606775999069214\n",
      "Epoch 371, Loss: -2.4012138843536377\n",
      "Epoch 372, Loss: -1.718267560005188\n",
      "Epoch 373, Loss: -1.6616274118423462\n",
      "Epoch 374, Loss: -2.3538835048675537\n",
      "Epoch 375, Loss: -1.8511449098587036\n",
      "Epoch 376, Loss: -1.7607882022857666\n",
      "Epoch 377, Loss: -1.3004356622695923\n",
      "Epoch 378, Loss: -1.716010570526123\n",
      "Epoch 379, Loss: -1.6410974264144897\n",
      "Epoch 380, Loss: -1.3258578777313232\n",
      "Epoch 381, Loss: -1.3427798748016357\n",
      "Epoch 382, Loss: -1.7350679636001587\n",
      "Epoch 383, Loss: -2.325139045715332\n",
      "Epoch 384, Loss: -1.2927510738372803\n",
      "Epoch 385, Loss: -1.4519999027252197\n",
      "Epoch 386, Loss: -1.7675408124923706\n",
      "Epoch 387, Loss: -1.1464594602584839\n",
      "Epoch 388, Loss: -2.07392954826355\n",
      "Epoch 389, Loss: -0.641878604888916\n",
      "Epoch 390, Loss: -1.8876858949661255\n",
      "Epoch 391, Loss: -1.1899170875549316\n",
      "Epoch 392, Loss: -1.4570469856262207\n",
      "Epoch 393, Loss: -1.2571287155151367\n",
      "Epoch 394, Loss: -0.7618468403816223\n",
      "Epoch 395, Loss: -1.5147202014923096\n",
      "Epoch 396, Loss: -1.1486726999282837\n",
      "Epoch 397, Loss: -0.9617822170257568\n",
      "Epoch 398, Loss: -1.29462468624115\n",
      "Epoch 399, Loss: -1.714191198348999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env_name, epochs, steps_per_epoch, gamma, tau, clip_param, policy_lr, value_lr)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_epoch):\n\u001b[1;32m     61\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     action_probs, value \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[1;32m     65\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     26\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[0;32m---> 27\u001b[0m     state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action_probs, state_value\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# from gymnasium import make\n",
    "import gymnasium as gym\n",
    "\n",
    "# Define the Actor-Critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return action_probs, state_value\n",
    "\n",
    "# Compute Generalized Advantage Estimation (GAE)\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "# Train the PPO agent\n",
    "def train(env_name, epochs=1000, steps_per_epoch=2048, gamma=0.99, tau=0.95, clip_param=0.2, policy_lr=3e-4, value_lr=1e-3):\n",
    "    # env = make(env_name)\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    network = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=policy_lr)\n",
    "    value_optimizer = optim.Adam(network.critic.parameters(), lr=value_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state, info = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        actions = []\n",
    "        states = []\n",
    "        for _ in range(steps_per_epoch):\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "            action_probs, value = network(state)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action.item())\n",
    "            log_prob = dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            masks.append(1-done)\n",
    "            actions.append(action)\n",
    "            states.append(state)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                state, info = env.reset()\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        _, next_value = network(next_state)\n",
    "        returns = compute_gae(next_value, rewards, masks, values, gamma, tau)\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        advantage = returns - values\n",
    "\n",
    "        # Update policy and value networks\n",
    "        _, new_values = network(states)\n",
    "        dist = Categorical(network.actor(states))\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - new_values).pow(2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {policy_loss.item()}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train('CartPole-v1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
