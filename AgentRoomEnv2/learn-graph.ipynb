{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example data (inputs and targets)\n",
    "X, Y = torch.randn((100, 10), requires_grad=False), torch.randn(100, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1031348705291748, 1.1771663427352905)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model parameters\n",
    "W1 = torch.randn(2, 10, requires_grad=True)\n",
    "W2 = torch.randn(2, requires_grad=True)\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.SGD([W1, W2], lr=learning_rate)\n",
    "\n",
    "loss_all = []\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    for i in range(Y.shape[0]):\n",
    "        y_pred1 = torch.matmul(W1, X[i])\n",
    "        y_pred2 = torch.matmul(W2, y_pred1)\n",
    "\n",
    "        loss = (y_pred2 - Y[i]).pow(2).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        loss_all.append(loss.detach().cpu().item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_all)\n",
    "loss_all[0], loss_all[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41202986240386963, 1.1577295064926147)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model parameters\n",
    "W1 = torch.randn(2, 10, requires_grad=True)\n",
    "W2 = torch.randn(2, requires_grad=True)\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.SGD([W1, W2], lr=learning_rate)\n",
    "\n",
    "loss_all = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    y_pred1_all = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        y_pred1 = torch.matmul(W1, X[i])\n",
    "        y_pred1_all.append(y_pred1)\n",
    "\n",
    "    for idx, y_pred1 in enumerate(y_pred1_all):\n",
    "        y_pred2 = torch.matmul(W2, y_pred1)\n",
    "\n",
    "        loss = (y_pred2 - Y[idx]).pow(2).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        loss_all.append(loss.detach().cpu().item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_all)\n",
    "loss_all[0], loss_all[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8857, grad_fn=<SumBackward0>)\n",
      "tensor(1.8001, grad_fn=<SumBackward0>)\n",
      "tensor(1.7187, grad_fn=<SumBackward0>)\n",
      "tensor(1.6413, grad_fn=<SumBackward0>)\n",
      "tensor(1.5676, grad_fn=<SumBackward0>)\n",
      "tensor(1.4974, grad_fn=<SumBackward0>)\n",
      "tensor(1.4306, grad_fn=<SumBackward0>)\n",
      "tensor(1.3670, grad_fn=<SumBackward0>)\n",
      "tensor(1.3065, grad_fn=<SumBackward0>)\n",
      "tensor(1.2488, grad_fn=<SumBackward0>)\n",
      "tensor(1.1939, grad_fn=<SumBackward0>)\n",
      "tensor(1.1415, grad_fn=<SumBackward0>)\n",
      "tensor(1.0917, grad_fn=<SumBackward0>)\n",
      "tensor(1.0442, grad_fn=<SumBackward0>)\n",
      "tensor(0.9989, grad_fn=<SumBackward0>)\n",
      "tensor(0.9557, grad_fn=<SumBackward0>)\n",
      "tensor(0.9146, grad_fn=<SumBackward0>)\n",
      "tensor(0.8753, grad_fn=<SumBackward0>)\n",
      "tensor(0.8379, grad_fn=<SumBackward0>)\n",
      "tensor(0.8022, grad_fn=<SumBackward0>)\n",
      "tensor(0.7682, grad_fn=<SumBackward0>)\n",
      "tensor(0.7357, grad_fn=<SumBackward0>)\n",
      "tensor(0.7047, grad_fn=<SumBackward0>)\n",
      "tensor(0.6751, grad_fn=<SumBackward0>)\n",
      "tensor(0.6469, grad_fn=<SumBackward0>)\n",
      "tensor(0.6199, grad_fn=<SumBackward0>)\n",
      "tensor(0.5942, grad_fn=<SumBackward0>)\n",
      "tensor(0.5696, grad_fn=<SumBackward0>)\n",
      "tensor(0.5461, grad_fn=<SumBackward0>)\n",
      "tensor(0.5237, grad_fn=<SumBackward0>)\n",
      "tensor(0.5023, grad_fn=<SumBackward0>)\n",
      "tensor(0.4818, grad_fn=<SumBackward0>)\n",
      "tensor(0.4623, grad_fn=<SumBackward0>)\n",
      "tensor(0.4436, grad_fn=<SumBackward0>)\n",
      "tensor(0.4257, grad_fn=<SumBackward0>)\n",
      "tensor(0.4086, grad_fn=<SumBackward0>)\n",
      "tensor(0.3923, grad_fn=<SumBackward0>)\n",
      "tensor(0.3766, grad_fn=<SumBackward0>)\n",
      "tensor(0.3617, grad_fn=<SumBackward0>)\n",
      "tensor(0.3474, grad_fn=<SumBackward0>)\n",
      "tensor(0.3337, grad_fn=<SumBackward0>)\n",
      "tensor(0.3206, grad_fn=<SumBackward0>)\n",
      "tensor(0.3081, grad_fn=<SumBackward0>)\n",
      "tensor(0.2961, grad_fn=<SumBackward0>)\n",
      "tensor(0.2846, grad_fn=<SumBackward0>)\n",
      "tensor(0.2736, grad_fn=<SumBackward0>)\n",
      "tensor(0.2630, grad_fn=<SumBackward0>)\n",
      "tensor(0.2530, grad_fn=<SumBackward0>)\n",
      "tensor(0.2433, grad_fn=<SumBackward0>)\n",
      "tensor(0.2340, grad_fn=<SumBackward0>)\n",
      "tensor(0.2251, grad_fn=<SumBackward0>)\n",
      "tensor(0.2166, grad_fn=<SumBackward0>)\n",
      "tensor(0.2085, grad_fn=<SumBackward0>)\n",
      "tensor(0.2006, grad_fn=<SumBackward0>)\n",
      "tensor(0.1931, grad_fn=<SumBackward0>)\n",
      "tensor(0.1860, grad_fn=<SumBackward0>)\n",
      "tensor(0.1791, grad_fn=<SumBackward0>)\n",
      "tensor(0.1724, grad_fn=<SumBackward0>)\n",
      "tensor(0.1661, grad_fn=<SumBackward0>)\n",
      "tensor(0.1600, grad_fn=<SumBackward0>)\n",
      "tensor(0.1541, grad_fn=<SumBackward0>)\n",
      "tensor(0.1485, grad_fn=<SumBackward0>)\n",
      "tensor(0.1431, grad_fn=<SumBackward0>)\n",
      "tensor(0.1380, grad_fn=<SumBackward0>)\n",
      "tensor(0.1330, grad_fn=<SumBackward0>)\n",
      "tensor(0.1282, grad_fn=<SumBackward0>)\n",
      "tensor(0.1236, grad_fn=<SumBackward0>)\n",
      "tensor(0.1192, grad_fn=<SumBackward0>)\n",
      "tensor(0.1150, grad_fn=<SumBackward0>)\n",
      "tensor(0.1109, grad_fn=<SumBackward0>)\n",
      "tensor(0.1070, grad_fn=<SumBackward0>)\n",
      "tensor(0.1032, grad_fn=<SumBackward0>)\n",
      "tensor(0.0996, grad_fn=<SumBackward0>)\n",
      "tensor(0.0961, grad_fn=<SumBackward0>)\n",
      "tensor(0.0928, grad_fn=<SumBackward0>)\n",
      "tensor(0.0896, grad_fn=<SumBackward0>)\n",
      "tensor(0.0865, grad_fn=<SumBackward0>)\n",
      "tensor(0.0835, grad_fn=<SumBackward0>)\n",
      "tensor(0.0806, grad_fn=<SumBackward0>)\n",
      "tensor(0.0779, grad_fn=<SumBackward0>)\n",
      "tensor(0.0752, grad_fn=<SumBackward0>)\n",
      "tensor(0.0727, grad_fn=<SumBackward0>)\n",
      "tensor(0.0702, grad_fn=<SumBackward0>)\n",
      "tensor(0.0678, grad_fn=<SumBackward0>)\n",
      "tensor(0.0656, grad_fn=<SumBackward0>)\n",
      "tensor(0.0634, grad_fn=<SumBackward0>)\n",
      "tensor(0.0612, grad_fn=<SumBackward0>)\n",
      "tensor(0.0592, grad_fn=<SumBackward0>)\n",
      "tensor(0.0572, grad_fn=<SumBackward0>)\n",
      "tensor(0.0553, grad_fn=<SumBackward0>)\n",
      "tensor(0.0535, grad_fn=<SumBackward0>)\n",
      "tensor(0.0518, grad_fn=<SumBackward0>)\n",
      "tensor(0.0501, grad_fn=<SumBackward0>)\n",
      "tensor(0.0484, grad_fn=<SumBackward0>)\n",
      "tensor(0.0468, grad_fn=<SumBackward0>)\n",
      "tensor(0.0453, grad_fn=<SumBackward0>)\n",
      "tensor(0.0438, grad_fn=<SumBackward0>)\n",
      "tensor(0.0424, grad_fn=<SumBackward0>)\n",
      "tensor(0.0411, grad_fn=<SumBackward0>)\n",
      "tensor(0.0397, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example data (inputs and targets)\n",
    "x1, y1 = torch.randn(10, requires_grad=False), torch.randn(1, requires_grad=False)\n",
    "x2, y2 = torch.randn(10, requires_grad=False), torch.randn(1, requires_grad=False)\n",
    "\n",
    "# Model parameters\n",
    "W = torch.randn(1, 10, requires_grad=True)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD([W], lr=learning_rate)\n",
    "# print(W)\n",
    "\n",
    "for _ in range(100):\n",
    "    y_pred1 = torch.matmul(W, x1)\n",
    "    y_pred2 = torch.matmul(W, x2)\n",
    "\n",
    "    loss = (y_pred1 - y1).pow(2).sum()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(W)\n",
    "\n",
    "    loss = (y_pred2 - y2).pow(2).sum()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(W)\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.1393, grad_fn=<SumBackward0>),\n",
       " tensor(2.0889, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6808598041534424, Accuracy: 0.5\n",
      "Epoch 10, Loss: 0.5601635575294495, Accuracy: 0.8333333333333334\n",
      "Epoch 20, Loss: 0.30671730637550354, Accuracy: 1.0\n",
      "Epoch 30, Loss: 0.07796495407819748, Accuracy: 1.0\n",
      "Epoch 40, Loss: 0.012967590242624283, Accuracy: 1.0\n",
      "Epoch 50, Loss: 0.0034221073146909475, Accuracy: 1.0\n",
      "Epoch 60, Loss: 0.0016223536804318428, Accuracy: 1.0\n",
      "Epoch 70, Loss: 0.0010926665272563696, Accuracy: 1.0\n",
      "Epoch 80, Loss: 0.0008700544130988419, Accuracy: 1.0\n",
      "Epoch 90, Loss: 0.0007468627882190049, Accuracy: 1.0\n",
      "Epoch 100, Loss: 0.0006629495765082538, Accuracy: 1.0\n",
      "Epoch 110, Loss: 0.0005976849352009594, Accuracy: 1.0\n",
      "Epoch 120, Loss: 0.0005431936588138342, Accuracy: 1.0\n",
      "Epoch 130, Loss: 0.0004961230442859232, Accuracy: 1.0\n",
      "Epoch 140, Loss: 0.0004548664146568626, Accuracy: 1.0\n",
      "Epoch 150, Loss: 0.00041831305134110153, Accuracy: 1.0\n",
      "Epoch 160, Loss: 0.000385788589483127, Accuracy: 1.0\n",
      "Epoch 170, Loss: 0.0003567773092072457, Accuracy: 1.0\n",
      "Epoch 180, Loss: 0.000330783223034814, Accuracy: 1.0\n",
      "Epoch 190, Loss: 0.00030738970963284373, Accuracy: 1.0\n",
      "Epoch 200, Loss: 0.00028635861235670745, Accuracy: 1.0\n",
      "Epoch 210, Loss: 0.0002673129492904991, Accuracy: 1.0\n",
      "Epoch 220, Loss: 0.0002500938717275858, Accuracy: 1.0\n",
      "Epoch 230, Loss: 0.00023440369113814086, Accuracy: 1.0\n",
      "Epoch 240, Loss: 0.00022018291929271072, Accuracy: 1.0\n",
      "Epoch 250, Loss: 0.0002071336639346555, Accuracy: 1.0\n",
      "Epoch 260, Loss: 0.00019523622177075595, Accuracy: 1.0\n",
      "Epoch 270, Loss: 0.00018427206669002771, Accuracy: 1.0\n",
      "Epoch 280, Loss: 0.00017424132965970784, Accuracy: 1.0\n",
      "Epoch 290, Loss: 0.00016500493802595884, Accuracy: 1.0\n",
      "Epoch 300, Loss: 0.00015642395010218024, Accuracy: 1.0\n",
      "Epoch 310, Loss: 0.00014853807806503028, Accuracy: 1.0\n",
      "Epoch 320, Loss: 0.00014120830746833235, Accuracy: 1.0\n",
      "Epoch 330, Loss: 0.00013439491158351302, Accuracy: 1.0\n",
      "Epoch 340, Loss: 0.00012807805615011603, Accuracy: 1.0\n",
      "Epoch 350, Loss: 0.00012219818017911166, Accuracy: 1.0\n",
      "Epoch 360, Loss: 0.00011669567174976692, Accuracy: 1.0\n",
      "Epoch 370, Loss: 0.00011153081868542358, Accuracy: 1.0\n",
      "Epoch 380, Loss: 0.00010674336226657033, Accuracy: 1.0\n",
      "Epoch 390, Loss: 0.00010223397839581594, Accuracy: 1.0\n",
      "Epoch 400, Loss: 9.800268890103325e-05, Accuracy: 1.0\n",
      "Epoch 410, Loss: 9.404949378222227e-05, Accuracy: 1.0\n",
      "Epoch 420, Loss: 9.033464448293671e-05, Accuracy: 1.0\n",
      "Epoch 430, Loss: 8.679857273818925e-05, Accuracy: 1.0\n",
      "Epoch 440, Loss: 8.346114191226661e-05, Accuracy: 1.0\n",
      "Epoch 450, Loss: 8.03422080934979e-05, Accuracy: 1.0\n",
      "Epoch 460, Loss: 7.73822030168958e-05, Accuracy: 1.0\n",
      "Epoch 470, Loss: 7.458110485458747e-05, Accuracy: 1.0\n",
      "Epoch 480, Loss: 7.195879152277485e-05, Accuracy: 1.0\n",
      "Epoch 490, Loss: 6.943579501239583e-05, Accuracy: 1.0\n",
      "Epoch 500, Loss: 6.707171996822581e-05, Accuracy: 1.0\n",
      "Epoch 510, Loss: 6.476723501691595e-05, Accuracy: 1.0\n",
      "Epoch 520, Loss: 6.262167153181508e-05, Accuracy: 1.0\n",
      "Epoch 530, Loss: 6.059529914637096e-05, Accuracy: 1.0\n",
      "Epoch 540, Loss: 5.862852049176581e-05, Accuracy: 1.0\n",
      "Epoch 550, Loss: 5.678093293681741e-05, Accuracy: 1.0\n",
      "Epoch 560, Loss: 5.4992950026644394e-05, Accuracy: 1.0\n",
      "Epoch 570, Loss: 5.330428757588379e-05, Accuracy: 1.0\n",
      "Epoch 580, Loss: 5.1695085858227685e-05, Accuracy: 1.0\n",
      "Epoch 590, Loss: 5.016535214963369e-05, Accuracy: 1.0\n",
      "Epoch 600, Loss: 4.869521580985747e-05, Accuracy: 1.0\n",
      "Epoch 610, Loss: 4.7284673200920224e-05, Accuracy: 1.0\n",
      "Epoch 620, Loss: 4.593373159877956e-05, Accuracy: 1.0\n",
      "Epoch 630, Loss: 4.4622516725212336e-05, Accuracy: 1.0\n",
      "Epoch 640, Loss: 4.3390766222728416e-05, Accuracy: 1.0\n",
      "Epoch 650, Loss: 4.219874972477555e-05, Accuracy: 1.0\n",
      "Epoch 660, Loss: 4.104647086933255e-05, Accuracy: 1.0\n",
      "Epoch 670, Loss: 3.9953778468770906e-05, Accuracy: 1.0\n",
      "Epoch 680, Loss: 3.890082734869793e-05, Accuracy: 1.0\n",
      "Epoch 690, Loss: 3.7887602957198396e-05, Accuracy: 1.0\n",
      "Epoch 700, Loss: 3.691411984618753e-05, Accuracy: 1.0\n",
      "Epoch 710, Loss: 3.596049646148458e-05, Accuracy: 1.0\n",
      "Epoch 720, Loss: 3.5066474083578214e-05, Accuracy: 1.0\n",
      "Epoch 730, Loss: 3.417245170567185e-05, Accuracy: 1.0\n",
      "Epoch 740, Loss: 3.335789733682759e-05, Accuracy: 1.0\n",
      "Epoch 750, Loss: 3.256320997024886e-05, Accuracy: 1.0\n",
      "Epoch 760, Loss: 3.176851896569133e-05, Accuracy: 1.0\n",
      "Epoch 770, Loss: 3.1033428967930377e-05, Accuracy: 1.0\n",
      "Epoch 780, Loss: 3.0298338970169425e-05, Accuracy: 1.0\n",
      "Epoch 790, Loss: 2.9602981157950126e-05, Accuracy: 1.0\n",
      "Epoch 800, Loss: 2.8927492166985758e-05, Accuracy: 1.0\n",
      "Epoch 810, Loss: 2.8251999538042583e-05, Accuracy: 1.0\n",
      "Epoch 820, Loss: 2.7616246370598674e-05, Accuracy: 1.0\n",
      "Epoch 830, Loss: 2.7000356567441486e-05, Accuracy: 1.0\n",
      "Epoch 840, Loss: 2.6424202587804757e-05, Accuracy: 1.0\n",
      "Epoch 850, Loss: 2.5828177967923693e-05, Accuracy: 1.0\n",
      "Epoch 860, Loss: 2.5311623176094145e-05, Accuracy: 1.0\n",
      "Epoch 870, Loss: 2.475533437973354e-05, Accuracy: 1.0\n",
      "Epoch 880, Loss: 2.4218912585638463e-05, Accuracy: 1.0\n",
      "Epoch 890, Loss: 2.3722226615063846e-05, Accuracy: 1.0\n",
      "Epoch 900, Loss: 2.3225538825499825e-05, Accuracy: 1.0\n",
      "Epoch 910, Loss: 2.2748718038201332e-05, Accuracy: 1.0\n",
      "Epoch 920, Loss: 2.2291764253168367e-05, Accuracy: 1.0\n",
      "Epoch 930, Loss: 2.185467747040093e-05, Accuracy: 1.0\n",
      "Epoch 940, Loss: 2.1397723685367964e-05, Accuracy: 1.0\n",
      "Epoch 950, Loss: 2.0980505723855458e-05, Accuracy: 1.0\n",
      "Epoch 960, Loss: 2.0583152945619076e-05, Accuracy: 1.0\n",
      "Epoch 970, Loss: 2.0165933165117167e-05, Accuracy: 1.0\n",
      "Epoch 980, Loss: 1.9788447389146313e-05, Accuracy: 1.0\n",
      "Epoch 990, Loss: 1.941096161317546e-05, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# Step 2: Define the GNN Model\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,num_node_features: int):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Step 1: Generate a Synthetic Graph\n",
    "# Node features\n",
    "x = torch.tensor([[1, 0], [0, 1], [1, 1], [1, 0]], dtype=torch.float)\n",
    "# Edges\n",
    "edge_index = torch.tensor([[0, 1, 2, 3, 0, 2],\n",
    "                           [1, 0, 3, 2, 2, 0]], dtype=torch.long)\n",
    "# Edge labels for classification\n",
    "edge_label = torch.tensor([0, 1, 0, 1, 1, 0], dtype=torch.long)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "model = GCN(num_node_features=data.num_node_features)\n",
    "\n",
    "# Edge classifier: A simple linear layer\n",
    "# Assuming we concatenate two node embeddings\n",
    "edge_classifier = torch.nn.Linear(16 * 2, 2)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(edge_classifier.parameters()), lr=0.01)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "for epoch in range(1000):\n",
    "    node_embeddings = model(data)\n",
    "\n",
    "    # node_embeddings.shape == torch.Size([4, 16])\n",
    "\n",
    "    # Get embeddings of both source and target nodes of each edge\n",
    "    source_embeddings = node_embeddings[data.edge_index[0]]\n",
    "    target_embeddings = node_embeddings[data.edge_index[1]]\n",
    "\n",
    "    # Edge classification: Here we concatenate source and target embeddings\n",
    "    edge_embeddings = torch.cat([source_embeddings, target_embeddings], dim=1)\n",
    "    edge_pred = edge_classifier(edge_embeddings)\n",
    "\n",
    "    loss = F.cross_entropy(edge_pred, edge_label)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    _, pred = torch.max(edge_pred, dim=1)\n",
    "    correct = pred.eq(edge_label).sum().item()\n",
    "    accuracy = correct / edge_label.size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7016306519508362, Accuracy: 0.5\n",
      "Epoch 10, Loss: 0.6459193825721741, Accuracy: 0.8333333333333334\n",
      "Epoch 20, Loss: 0.42045748233795166, Accuracy: 0.6666666666666666\n",
      "Epoch 30, Loss: 0.08629082888364792, Accuracy: 1.0\n",
      "Epoch 40, Loss: 0.0052810912020504475, Accuracy: 1.0\n",
      "Epoch 50, Loss: 0.00060127186588943, Accuracy: 1.0\n",
      "Epoch 60, Loss: 0.00018166076915804297, Accuracy: 1.0\n",
      "Epoch 70, Loss: 0.00010940333595499396, Accuracy: 1.0\n",
      "Epoch 80, Loss: 8.832736784825101e-05, Accuracy: 1.0\n",
      "Epoch 90, Loss: 7.98052569734864e-05, Accuracy: 1.0\n",
      "Epoch 100, Loss: 7.515673496527597e-05, Accuracy: 1.0\n",
      "Epoch 110, Loss: 7.18192895874381e-05, Accuracy: 1.0\n",
      "Epoch 120, Loss: 6.905792542966083e-05, Accuracy: 1.0\n",
      "Epoch 130, Loss: 6.649520219070837e-05, Accuracy: 1.0\n",
      "Epoch 140, Loss: 6.40715443296358e-05, Accuracy: 1.0\n",
      "Epoch 150, Loss: 6.174719601403922e-05, Accuracy: 1.0\n",
      "Epoch 160, Loss: 5.948244870523922e-05, Accuracy: 1.0\n",
      "Epoch 170, Loss: 5.731702549383044e-05, Accuracy: 1.0\n",
      "Epoch 180, Loss: 5.5310516472673044e-05, Accuracy: 1.0\n",
      "Epoch 190, Loss: 5.344307282939553e-05, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Synthetic Graph Data Setup (including node features, edges, and edge labels)\n",
    "x = torch.tensor([[1, 0], [0, 1], [1, 1], [1, 0]],\n",
    "                 dtype=torch.float)  # Node features\n",
    "edge_index = torch.tensor(\n",
    "    [[0, 1, 2, 3, 0, 2], [1, 0, 3, 2, 2, 0]], dtype=torch.long\n",
    ")  # Edges\n",
    "edge_label = torch.tensor(\n",
    "    [0, 1, 0, 1, 1, 0], dtype=torch.long\n",
    ")  # Edge labels for classification\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "# EnhancedGCN Model Definition with Learnable Embeddings\n",
    "class EnhancedGCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_node_features, num_embedding_features):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(num_nodes, num_embedding_features)\n",
    "        self.embeddings.weight.data.uniform_(0, 1)  # Initialize embeddings\n",
    "\n",
    "        self.conv1 = GCNConv(num_embedding_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        node_indices = torch.arange(\n",
    "            0, x.size(0), dtype=torch.long, device=x.device)\n",
    "        node_embeddings = self.embeddings(node_indices)\n",
    "\n",
    "        # x = torch.cat([x, node_embeddings], dim=1)  # Concatenate original features with embeddings\n",
    "        # x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv1(node_embeddings, edge_index))\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model and Optimizer Setup\n",
    "num_nodes = data.num_nodes\n",
    "num_node_features = data.num_features\n",
    "num_embedding_features = 5  # Example additional embedding size\n",
    "model = EnhancedGCN(num_nodes, num_node_features, num_embedding_features)\n",
    "\n",
    "edge_classifier = torch.nn.Linear(\n",
    "    16 * 2, 2\n",
    ")  # Classifier for the concatenated node embeddings\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(edge_classifier.parameters()), lr=0.01\n",
    ")\n",
    "\n",
    "# Training Loop with Accuracy Calculation for Edge Classification\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    node_embeddings = model(data)\n",
    "\n",
    "    source_embeddings = node_embeddings[data.edge_index[0]]\n",
    "    target_embeddings = node_embeddings[data.edge_index[1]]\n",
    "    edge_embeddings = torch.cat([source_embeddings, target_embeddings], dim=1)\n",
    "\n",
    "    edge_pred = edge_classifier(edge_embeddings)\n",
    "    loss = F.cross_entropy(edge_pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    _, pred = torch.max(edge_pred, dim=1)\n",
    "    correct = pred.eq(edge_label).sum().item()\n",
    "    accuracy = correct / edge_label.size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7564857602119446, Accuracy: 0.5\n",
      "Epoch 10, Loss: 0.6708261370658875, Accuracy: 0.5\n",
      "Epoch 20, Loss: 0.5375721454620361, Accuracy: 1.0\n",
      "Epoch 30, Loss: 0.2010474056005478, Accuracy: 1.0\n",
      "Epoch 40, Loss: 0.02258077822625637, Accuracy: 1.0\n",
      "Epoch 50, Loss: 0.0017681272001937032, Accuracy: 1.0\n",
      "Epoch 60, Loss: 0.0004274191742297262, Accuracy: 1.0\n",
      "Epoch 70, Loss: 0.00013457309978548437, Accuracy: 1.0\n",
      "Epoch 80, Loss: 4.569508018903434e-05, Accuracy: 1.0\n",
      "Epoch 90, Loss: 1.9251956473453902e-05, Accuracy: 1.0\n",
      "Epoch 100, Loss: 1.0351240234740544e-05, Accuracy: 1.0\n",
      "Epoch 110, Loss: 6.7551518441177905e-06, Accuracy: 1.0\n",
      "Epoch 120, Loss: 5.026635790272849e-06, Accuracy: 1.0\n",
      "Epoch 130, Loss: 3.993497557530645e-06, Accuracy: 1.0\n",
      "Epoch 140, Loss: 3.3179824185936013e-06, Accuracy: 1.0\n",
      "Epoch 150, Loss: 2.841147761500906e-06, Accuracy: 1.0\n",
      "Epoch 160, Loss: 2.463653572704061e-06, Accuracy: 1.0\n",
      "Epoch 170, Loss: 2.1656312583218096e-06, Accuracy: 1.0\n",
      "Epoch 180, Loss: 1.9272138160886243e-06, Accuracy: 1.0\n",
      "Epoch 190, Loss: 1.7086640582419932e-06, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class EnhancedGCNWithEdgeClassification(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_node_features, num_embedding_features, out_channels):\n",
    "        super(EnhancedGCNWithEdgeClassification, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(num_nodes, num_embedding_features)\n",
    "        self.embeddings.weight.data.uniform_(0, 1)  # Initialize embeddings\n",
    "\n",
    "        self.conv1 = GCNConv(num_node_features + num_embedding_features, 16)\n",
    "        self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "        # Instead of a separate edge classifier, incorporate it into the GNN model\n",
    "        # Assuming binary classification for edges\n",
    "        self.edge_classifier = torch.nn.Linear(out_channels * 2, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        node_indices = torch.arange(\n",
    "            0, x.size(0), dtype=torch.long, device=x.device)\n",
    "        node_embeddings = self.embeddings(node_indices)\n",
    "        # Concatenate original features with embeddings\n",
    "        x = torch.cat([x, node_embeddings], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # Now, for each edge, use the node embeddings to predict edge labels\n",
    "        source_embeddings = x[edge_index[0]]\n",
    "        target_embeddings = x[edge_index[1]]\n",
    "        edge_embeddings = torch.cat(\n",
    "            [source_embeddings, target_embeddings], dim=1)\n",
    "        edge_pred = self.edge_classifier(edge_embeddings)\n",
    "\n",
    "        return edge_pred\n",
    "\n",
    "\n",
    "# Parameters and Data Initialization\n",
    "num_nodes = data.num_nodes\n",
    "num_node_features = data.num_features\n",
    "num_embedding_features = 5  # Example additional embedding size\n",
    "out_channels = 16  # Example size for the node embeddings used in edge predictions\n",
    "\n",
    "model = EnhancedGCNWithEdgeClassification(\n",
    "    num_nodes, num_node_features, num_embedding_features, out_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Assume edge_index, x, and edge_label are defined as in the previous examples\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Training Loop with the Adjusted Model\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    edge_pred = model(data)  # The model now directly outputs edge predictions\n",
    "    loss = F.cross_entropy(edge_pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    _, pred = torch.max(edge_pred, dim=1)\n",
    "    correct = pred.eq(edge_label).sum().item()\n",
    "    accuracy = correct / edge_label.size(0)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -7.742973327636719\n",
      "Epoch 1, Loss: -7.680062294006348\n",
      "Epoch 2, Loss: -7.813106536865234\n",
      "Epoch 3, Loss: -7.8224077224731445\n",
      "Epoch 4, Loss: -7.743018627166748\n",
      "Epoch 5, Loss: -8.202510833740234\n",
      "Epoch 6, Loss: -7.553354740142822\n",
      "Epoch 7, Loss: -8.010868072509766\n",
      "Epoch 8, Loss: -7.105863571166992\n",
      "Epoch 9, Loss: -8.162364959716797\n",
      "Epoch 10, Loss: -7.905317783355713\n",
      "Epoch 11, Loss: -7.111297130584717\n",
      "Epoch 12, Loss: -7.412032127380371\n",
      "Epoch 13, Loss: -7.288893222808838\n",
      "Epoch 14, Loss: -7.157147407531738\n",
      "Epoch 15, Loss: -7.519998073577881\n",
      "Epoch 16, Loss: -7.834115505218506\n",
      "Epoch 17, Loss: -8.216315269470215\n",
      "Epoch 18, Loss: -7.244935512542725\n",
      "Epoch 19, Loss: -7.591847896575928\n",
      "Epoch 20, Loss: -7.370690822601318\n",
      "Epoch 21, Loss: -7.748913288116455\n",
      "Epoch 22, Loss: -7.396695137023926\n",
      "Epoch 23, Loss: -8.078722953796387\n",
      "Epoch 24, Loss: -7.408994674682617\n",
      "Epoch 25, Loss: -7.649755477905273\n",
      "Epoch 26, Loss: -7.609882831573486\n",
      "Epoch 27, Loss: -7.472285270690918\n",
      "Epoch 28, Loss: -7.769733428955078\n",
      "Epoch 29, Loss: -7.901911735534668\n",
      "Epoch 30, Loss: -7.156304836273193\n",
      "Epoch 31, Loss: -7.537949085235596\n",
      "Epoch 32, Loss: -7.1531548500061035\n",
      "Epoch 33, Loss: -7.365673542022705\n",
      "Epoch 34, Loss: -7.215046405792236\n",
      "Epoch 35, Loss: -6.8748297691345215\n",
      "Epoch 36, Loss: -7.534071922302246\n",
      "Epoch 37, Loss: -6.971118450164795\n",
      "Epoch 38, Loss: -7.179816246032715\n",
      "Epoch 39, Loss: -7.8539628982543945\n",
      "Epoch 40, Loss: -7.531057834625244\n",
      "Epoch 41, Loss: -7.103774070739746\n",
      "Epoch 42, Loss: -7.302676677703857\n",
      "Epoch 43, Loss: -7.202498435974121\n",
      "Epoch 44, Loss: -7.7534332275390625\n",
      "Epoch 45, Loss: -7.691716194152832\n",
      "Epoch 46, Loss: -7.624504089355469\n",
      "Epoch 47, Loss: -7.299200057983398\n",
      "Epoch 48, Loss: -7.185403823852539\n",
      "Epoch 49, Loss: -6.940850734710693\n",
      "Epoch 50, Loss: -7.512860298156738\n",
      "Epoch 51, Loss: -7.157609462738037\n",
      "Epoch 52, Loss: -6.897226810455322\n",
      "Epoch 53, Loss: -7.3367509841918945\n",
      "Epoch 54, Loss: -7.080990314483643\n",
      "Epoch 55, Loss: -7.133580207824707\n",
      "Epoch 56, Loss: -7.406336784362793\n",
      "Epoch 57, Loss: -6.71673583984375\n",
      "Epoch 58, Loss: -6.720027446746826\n",
      "Epoch 59, Loss: -6.438836097717285\n",
      "Epoch 60, Loss: -7.2160468101501465\n",
      "Epoch 61, Loss: -7.050616264343262\n",
      "Epoch 62, Loss: -7.25382137298584\n",
      "Epoch 63, Loss: -6.925450325012207\n",
      "Epoch 64, Loss: -7.0778985023498535\n",
      "Epoch 65, Loss: -6.784339904785156\n",
      "Epoch 66, Loss: -6.296026706695557\n",
      "Epoch 67, Loss: -7.239380836486816\n",
      "Epoch 68, Loss: -7.294602394104004\n",
      "Epoch 69, Loss: -7.1991868019104\n",
      "Epoch 70, Loss: -7.544665336608887\n",
      "Epoch 71, Loss: -6.836098670959473\n",
      "Epoch 72, Loss: -7.001278400421143\n",
      "Epoch 73, Loss: -6.472702980041504\n",
      "Epoch 74, Loss: -6.912676811218262\n",
      "Epoch 75, Loss: -7.029498100280762\n",
      "Epoch 76, Loss: -6.908527374267578\n",
      "Epoch 77, Loss: -7.032842636108398\n",
      "Epoch 78, Loss: -6.615264415740967\n",
      "Epoch 79, Loss: -7.01414155960083\n",
      "Epoch 80, Loss: -6.761105537414551\n",
      "Epoch 81, Loss: -6.5970964431762695\n",
      "Epoch 82, Loss: -6.920534133911133\n",
      "Epoch 83, Loss: -6.979144096374512\n",
      "Epoch 84, Loss: -6.628958702087402\n",
      "Epoch 85, Loss: -6.844524383544922\n",
      "Epoch 86, Loss: -7.014463424682617\n",
      "Epoch 87, Loss: -6.8155436515808105\n",
      "Epoch 88, Loss: -6.274809837341309\n",
      "Epoch 89, Loss: -6.802736282348633\n",
      "Epoch 90, Loss: -6.7133893966674805\n",
      "Epoch 91, Loss: -6.482316017150879\n",
      "Epoch 92, Loss: -6.706389904022217\n",
      "Epoch 93, Loss: -6.46449613571167\n",
      "Epoch 94, Loss: -6.548506736755371\n",
      "Epoch 95, Loss: -6.810980319976807\n",
      "Epoch 96, Loss: -6.686854362487793\n",
      "Epoch 97, Loss: -6.493123531341553\n",
      "Epoch 98, Loss: -6.589921951293945\n",
      "Epoch 99, Loss: -7.262610912322998\n",
      "Epoch 100, Loss: -5.769769191741943\n",
      "Epoch 101, Loss: -7.203604698181152\n",
      "Epoch 102, Loss: -6.5424699783325195\n",
      "Epoch 103, Loss: -6.477278709411621\n",
      "Epoch 104, Loss: -6.27017068862915\n",
      "Epoch 105, Loss: -6.531923770904541\n",
      "Epoch 106, Loss: -6.598543167114258\n",
      "Epoch 107, Loss: -6.786084175109863\n",
      "Epoch 108, Loss: -6.741419315338135\n",
      "Epoch 109, Loss: -6.0063629150390625\n",
      "Epoch 110, Loss: -6.604098320007324\n",
      "Epoch 111, Loss: -6.008785247802734\n",
      "Epoch 112, Loss: -6.352872848510742\n",
      "Epoch 113, Loss: -6.39543342590332\n",
      "Epoch 114, Loss: -6.701353549957275\n",
      "Epoch 115, Loss: -6.424685478210449\n",
      "Epoch 116, Loss: -6.190312385559082\n",
      "Epoch 117, Loss: -6.131669521331787\n",
      "Epoch 118, Loss: -6.0375518798828125\n",
      "Epoch 119, Loss: -6.023972034454346\n",
      "Epoch 120, Loss: -5.66478967666626\n",
      "Epoch 121, Loss: -6.339178085327148\n",
      "Epoch 122, Loss: -6.312556266784668\n",
      "Epoch 123, Loss: -5.659755229949951\n",
      "Epoch 124, Loss: -6.1858954429626465\n",
      "Epoch 125, Loss: -7.009976863861084\n",
      "Epoch 126, Loss: -5.759288787841797\n",
      "Epoch 127, Loss: -6.323179244995117\n",
      "Epoch 128, Loss: -5.780800819396973\n",
      "Epoch 129, Loss: -5.822617530822754\n",
      "Epoch 130, Loss: -5.679484844207764\n",
      "Epoch 131, Loss: -5.517055511474609\n",
      "Epoch 132, Loss: -5.522689342498779\n",
      "Epoch 133, Loss: -6.243618011474609\n",
      "Epoch 134, Loss: -5.806011199951172\n",
      "Epoch 135, Loss: -5.942624568939209\n",
      "Epoch 136, Loss: -6.336615085601807\n",
      "Epoch 137, Loss: -5.762424468994141\n",
      "Epoch 138, Loss: -5.840810298919678\n",
      "Epoch 139, Loss: -5.783056259155273\n",
      "Epoch 140, Loss: -5.6719136238098145\n",
      "Epoch 141, Loss: -5.798946857452393\n",
      "Epoch 142, Loss: -5.637406349182129\n",
      "Epoch 143, Loss: -6.11299991607666\n",
      "Epoch 144, Loss: -5.53279972076416\n",
      "Epoch 145, Loss: -6.311362266540527\n",
      "Epoch 146, Loss: -5.9792046546936035\n",
      "Epoch 147, Loss: -5.9610981941223145\n",
      "Epoch 148, Loss: -5.68293571472168\n",
      "Epoch 149, Loss: -5.645452499389648\n",
      "Epoch 150, Loss: -5.729876518249512\n",
      "Epoch 151, Loss: -5.483315944671631\n",
      "Epoch 152, Loss: -5.470064163208008\n",
      "Epoch 153, Loss: -5.532284259796143\n",
      "Epoch 154, Loss: -5.273942947387695\n",
      "Epoch 155, Loss: -5.334791660308838\n",
      "Epoch 156, Loss: -6.043630123138428\n",
      "Epoch 157, Loss: -4.89894437789917\n",
      "Epoch 158, Loss: -5.820525646209717\n",
      "Epoch 159, Loss: -5.438302040100098\n",
      "Epoch 160, Loss: -5.682138442993164\n",
      "Epoch 161, Loss: -5.787511348724365\n",
      "Epoch 162, Loss: -5.519470691680908\n",
      "Epoch 163, Loss: -4.713095664978027\n",
      "Epoch 164, Loss: -5.971782684326172\n",
      "Epoch 165, Loss: -5.477962017059326\n",
      "Epoch 166, Loss: -5.000701904296875\n",
      "Epoch 167, Loss: -5.741398811340332\n",
      "Epoch 168, Loss: -5.239965915679932\n",
      "Epoch 169, Loss: -5.693813800811768\n",
      "Epoch 170, Loss: -5.381564140319824\n",
      "Epoch 171, Loss: -5.637574672698975\n",
      "Epoch 172, Loss: -5.0807929039001465\n",
      "Epoch 173, Loss: -5.373555660247803\n",
      "Epoch 174, Loss: -5.046562194824219\n",
      "Epoch 175, Loss: -5.682003498077393\n",
      "Epoch 176, Loss: -5.3481125831604\n",
      "Epoch 177, Loss: -4.825918674468994\n",
      "Epoch 178, Loss: -5.0473809242248535\n",
      "Epoch 179, Loss: -4.2640581130981445\n",
      "Epoch 180, Loss: -5.245913505554199\n",
      "Epoch 181, Loss: -4.95863151550293\n",
      "Epoch 182, Loss: -4.936344146728516\n",
      "Epoch 183, Loss: -4.98270320892334\n",
      "Epoch 184, Loss: -4.5337419509887695\n",
      "Epoch 185, Loss: -4.958657741546631\n",
      "Epoch 186, Loss: -5.086697578430176\n",
      "Epoch 187, Loss: -4.782310962677002\n",
      "Epoch 188, Loss: -5.405375957489014\n",
      "Epoch 189, Loss: -4.898563385009766\n",
      "Epoch 190, Loss: -5.043748378753662\n",
      "Epoch 191, Loss: -4.351369857788086\n",
      "Epoch 192, Loss: -5.221548080444336\n",
      "Epoch 193, Loss: -5.071567535400391\n",
      "Epoch 194, Loss: -4.511063575744629\n",
      "Epoch 195, Loss: -4.747114181518555\n",
      "Epoch 196, Loss: -4.659502983093262\n",
      "Epoch 197, Loss: -4.885140419006348\n",
      "Epoch 198, Loss: -4.667304992675781\n",
      "Epoch 199, Loss: -4.645142078399658\n",
      "Epoch 200, Loss: -4.750586986541748\n",
      "Epoch 201, Loss: -5.149101257324219\n",
      "Epoch 202, Loss: -4.324589729309082\n",
      "Epoch 203, Loss: -4.378162860870361\n",
      "Epoch 204, Loss: -5.083069324493408\n",
      "Epoch 205, Loss: -4.252825736999512\n",
      "Epoch 206, Loss: -5.065377235412598\n",
      "Epoch 207, Loss: -5.396033763885498\n",
      "Epoch 208, Loss: -3.896573781967163\n",
      "Epoch 209, Loss: -4.714521408081055\n",
      "Epoch 210, Loss: -4.249679088592529\n",
      "Epoch 211, Loss: -4.999024868011475\n",
      "Epoch 212, Loss: -4.333959579467773\n",
      "Epoch 213, Loss: -4.462892532348633\n",
      "Epoch 214, Loss: -4.524807453155518\n",
      "Epoch 215, Loss: -4.881594657897949\n",
      "Epoch 216, Loss: -4.589057445526123\n",
      "Epoch 217, Loss: -4.2467803955078125\n",
      "Epoch 218, Loss: -4.330737113952637\n",
      "Epoch 219, Loss: -3.9991447925567627\n",
      "Epoch 220, Loss: -4.601106643676758\n",
      "Epoch 221, Loss: -3.8788416385650635\n",
      "Epoch 222, Loss: -4.204153537750244\n",
      "Epoch 223, Loss: -3.8329503536224365\n",
      "Epoch 224, Loss: -4.310824394226074\n",
      "Epoch 225, Loss: -3.6943612098693848\n",
      "Epoch 226, Loss: -4.049969673156738\n",
      "Epoch 227, Loss: -4.092465877532959\n",
      "Epoch 228, Loss: -3.98799991607666\n",
      "Epoch 229, Loss: -4.037148952484131\n",
      "Epoch 230, Loss: -4.370362281799316\n",
      "Epoch 231, Loss: -4.355512619018555\n",
      "Epoch 232, Loss: -4.105466365814209\n",
      "Epoch 233, Loss: -3.7499260902404785\n",
      "Epoch 234, Loss: -3.60827898979187\n",
      "Epoch 235, Loss: -4.028979301452637\n",
      "Epoch 236, Loss: -3.6669816970825195\n",
      "Epoch 237, Loss: -3.7647852897644043\n",
      "Epoch 238, Loss: -3.75938081741333\n",
      "Epoch 239, Loss: -3.7816734313964844\n",
      "Epoch 240, Loss: -4.016806602478027\n",
      "Epoch 241, Loss: -3.6187448501586914\n",
      "Epoch 242, Loss: -3.6800265312194824\n",
      "Epoch 243, Loss: -3.755857467651367\n",
      "Epoch 244, Loss: -3.9251468181610107\n",
      "Epoch 245, Loss: -3.472487449645996\n",
      "Epoch 246, Loss: -3.365023374557495\n",
      "Epoch 247, Loss: -3.259828805923462\n",
      "Epoch 248, Loss: -3.626984119415283\n",
      "Epoch 249, Loss: -4.005743980407715\n",
      "Epoch 250, Loss: -3.4176862239837646\n",
      "Epoch 251, Loss: -2.492014169692993\n",
      "Epoch 252, Loss: -3.18989634513855\n",
      "Epoch 253, Loss: -3.513371467590332\n",
      "Epoch 254, Loss: -4.144147872924805\n",
      "Epoch 255, Loss: -3.691070556640625\n",
      "Epoch 256, Loss: -3.0424675941467285\n",
      "Epoch 257, Loss: -3.7162506580352783\n",
      "Epoch 258, Loss: -3.1843388080596924\n",
      "Epoch 259, Loss: -2.638047933578491\n",
      "Epoch 260, Loss: -3.5998289585113525\n",
      "Epoch 261, Loss: -2.803983211517334\n",
      "Epoch 262, Loss: -4.088169097900391\n",
      "Epoch 263, Loss: -3.4317517280578613\n",
      "Epoch 264, Loss: -3.8210721015930176\n",
      "Epoch 265, Loss: -3.6417417526245117\n",
      "Epoch 266, Loss: -3.5454602241516113\n",
      "Epoch 267, Loss: -3.1075448989868164\n",
      "Epoch 268, Loss: -3.089751720428467\n",
      "Epoch 269, Loss: -3.7131757736206055\n",
      "Epoch 270, Loss: -3.264967679977417\n",
      "Epoch 271, Loss: -3.75939679145813\n",
      "Epoch 272, Loss: -3.2778661251068115\n",
      "Epoch 273, Loss: -3.059882164001465\n",
      "Epoch 274, Loss: -2.7667624950408936\n",
      "Epoch 275, Loss: -2.6517856121063232\n",
      "Epoch 276, Loss: -2.836699962615967\n",
      "Epoch 277, Loss: -3.0394163131713867\n",
      "Epoch 278, Loss: -2.6698780059814453\n",
      "Epoch 279, Loss: -3.4001269340515137\n",
      "Epoch 280, Loss: -3.097851276397705\n",
      "Epoch 281, Loss: -2.5173099040985107\n",
      "Epoch 282, Loss: -2.669553756713867\n",
      "Epoch 283, Loss: -3.526226043701172\n",
      "Epoch 284, Loss: -3.207054376602173\n",
      "Epoch 285, Loss: -3.2666850090026855\n",
      "Epoch 286, Loss: -3.4172441959381104\n",
      "Epoch 287, Loss: -3.113588333129883\n",
      "Epoch 288, Loss: -2.600125789642334\n",
      "Epoch 289, Loss: -3.206317901611328\n",
      "Epoch 290, Loss: -3.0334434509277344\n",
      "Epoch 291, Loss: -3.2108144760131836\n",
      "Epoch 292, Loss: -2.787569046020508\n",
      "Epoch 293, Loss: -3.189366579055786\n",
      "Epoch 294, Loss: -3.2981057167053223\n",
      "Epoch 295, Loss: -3.1810801029205322\n",
      "Epoch 296, Loss: -2.5116724967956543\n",
      "Epoch 297, Loss: -3.3740153312683105\n",
      "Epoch 298, Loss: -2.8255395889282227\n",
      "Epoch 299, Loss: -2.873944044113159\n",
      "Epoch 300, Loss: -2.72174072265625\n",
      "Epoch 301, Loss: -2.3557791709899902\n",
      "Epoch 302, Loss: -2.4161834716796875\n",
      "Epoch 303, Loss: -2.6591978073120117\n",
      "Epoch 304, Loss: -2.92646861076355\n",
      "Epoch 305, Loss: -2.570192337036133\n",
      "Epoch 306, Loss: -2.26039981842041\n",
      "Epoch 307, Loss: -2.8369429111480713\n",
      "Epoch 308, Loss: -2.1248886585235596\n",
      "Epoch 309, Loss: -2.7164156436920166\n",
      "Epoch 310, Loss: -3.097752332687378\n",
      "Epoch 311, Loss: -1.440101981163025\n",
      "Epoch 312, Loss: -2.9090476036071777\n",
      "Epoch 313, Loss: -2.767737627029419\n",
      "Epoch 314, Loss: -3.0423920154571533\n",
      "Epoch 315, Loss: -2.226839065551758\n",
      "Epoch 316, Loss: -2.095813274383545\n",
      "Epoch 317, Loss: -2.291487216949463\n",
      "Epoch 318, Loss: -2.5734047889709473\n",
      "Epoch 319, Loss: -2.1734750270843506\n",
      "Epoch 320, Loss: -2.4258406162261963\n",
      "Epoch 321, Loss: -2.309406042098999\n",
      "Epoch 322, Loss: -2.579927444458008\n",
      "Epoch 323, Loss: -2.2809219360351562\n",
      "Epoch 324, Loss: -2.6724562644958496\n",
      "Epoch 325, Loss: -2.593019962310791\n",
      "Epoch 326, Loss: -2.2472939491271973\n",
      "Epoch 327, Loss: -2.4273977279663086\n",
      "Epoch 328, Loss: -2.667393207550049\n",
      "Epoch 329, Loss: -2.718330144882202\n",
      "Epoch 330, Loss: -2.2645680904388428\n",
      "Epoch 331, Loss: -2.6979458332061768\n",
      "Epoch 332, Loss: -2.8205273151397705\n",
      "Epoch 333, Loss: -1.9479596614837646\n",
      "Epoch 334, Loss: -2.420680284500122\n",
      "Epoch 335, Loss: -2.146177291870117\n",
      "Epoch 336, Loss: -2.7258949279785156\n",
      "Epoch 337, Loss: -2.7547974586486816\n",
      "Epoch 338, Loss: -1.92744779586792\n",
      "Epoch 339, Loss: -2.3911285400390625\n",
      "Epoch 340, Loss: -2.2666049003601074\n",
      "Epoch 341, Loss: -2.2441086769104004\n",
      "Epoch 342, Loss: -2.371751546859741\n",
      "Epoch 343, Loss: -1.814414143562317\n",
      "Epoch 344, Loss: -2.420276165008545\n",
      "Epoch 345, Loss: -1.879414677619934\n",
      "Epoch 346, Loss: -1.8881416320800781\n",
      "Epoch 347, Loss: -1.7762770652770996\n",
      "Epoch 348, Loss: -1.5826168060302734\n",
      "Epoch 349, Loss: -3.4404776096343994\n",
      "Epoch 350, Loss: -1.8099415302276611\n",
      "Epoch 351, Loss: -1.8284411430358887\n",
      "Epoch 352, Loss: -1.6922693252563477\n",
      "Epoch 353, Loss: -1.6398303508758545\n",
      "Epoch 354, Loss: -1.51090407371521\n",
      "Epoch 355, Loss: -1.6870877742767334\n",
      "Epoch 356, Loss: -1.362095832824707\n",
      "Epoch 357, Loss: -2.1804251670837402\n",
      "Epoch 358, Loss: -1.6504772901535034\n",
      "Epoch 359, Loss: -1.9313498735427856\n",
      "Epoch 360, Loss: -1.666489601135254\n",
      "Epoch 361, Loss: -1.9092345237731934\n",
      "Epoch 362, Loss: -2.0991437435150146\n",
      "Epoch 363, Loss: -1.7148799896240234\n",
      "Epoch 364, Loss: -1.6707013845443726\n",
      "Epoch 365, Loss: -1.57049560546875\n",
      "Epoch 366, Loss: -1.7201833724975586\n",
      "Epoch 367, Loss: -1.410898208618164\n",
      "Epoch 368, Loss: -2.3576810359954834\n",
      "Epoch 369, Loss: -2.1398894786834717\n",
      "Epoch 370, Loss: -1.8606775999069214\n",
      "Epoch 371, Loss: -2.4012138843536377\n",
      "Epoch 372, Loss: -1.718267560005188\n",
      "Epoch 373, Loss: -1.6616274118423462\n",
      "Epoch 374, Loss: -2.3538835048675537\n",
      "Epoch 375, Loss: -1.8511449098587036\n",
      "Epoch 376, Loss: -1.7607882022857666\n",
      "Epoch 377, Loss: -1.3004356622695923\n",
      "Epoch 378, Loss: -1.716010570526123\n",
      "Epoch 379, Loss: -1.6410974264144897\n",
      "Epoch 380, Loss: -1.3258578777313232\n",
      "Epoch 381, Loss: -1.3427798748016357\n",
      "Epoch 382, Loss: -1.7350679636001587\n",
      "Epoch 383, Loss: -2.325139045715332\n",
      "Epoch 384, Loss: -1.2927510738372803\n",
      "Epoch 385, Loss: -1.4519999027252197\n",
      "Epoch 386, Loss: -1.7675408124923706\n",
      "Epoch 387, Loss: -1.1464594602584839\n",
      "Epoch 388, Loss: -2.07392954826355\n",
      "Epoch 389, Loss: -0.641878604888916\n",
      "Epoch 390, Loss: -1.8876858949661255\n",
      "Epoch 391, Loss: -1.1899170875549316\n",
      "Epoch 392, Loss: -1.4570469856262207\n",
      "Epoch 393, Loss: -1.2571287155151367\n",
      "Epoch 394, Loss: -0.7618468403816223\n",
      "Epoch 395, Loss: -1.5147202014923096\n",
      "Epoch 396, Loss: -1.1486726999282837\n",
      "Epoch 397, Loss: -0.9617822170257568\n",
      "Epoch 398, Loss: -1.29462468624115\n",
      "Epoch 399, Loss: -1.714191198348999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env_name, epochs, steps_per_epoch, gamma, tau, clip_param, policy_lr, value_lr)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_epoch):\n\u001b[1;32m     61\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     action_probs, value \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[1;32m     65\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     26\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[0;32m---> 27\u001b[0m     state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action_probs, state_value\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/modules/activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/human-memory/lib/python3.10/site-packages/torch/nn/functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# from gymnasium import make\n",
    "import gymnasium as gym\n",
    "\n",
    "# Define the Actor-Critic network\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return action_probs, state_value\n",
    "\n",
    "# Compute Generalized Advantage Estimation (GAE)\n",
    "\n",
    "\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * \\\n",
    "            values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "# Train the PPO agent\n",
    "\n",
    "\n",
    "def train(env_name, epochs=1000, steps_per_epoch=2048, gamma=0.99, tau=0.95, clip_param=0.2, policy_lr=3e-4, value_lr=1e-3):\n",
    "    # env = make(env_name)\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    network = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=policy_lr)\n",
    "    value_optimizer = optim.Adam(network.critic.parameters(), lr=value_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state, info = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        actions = []\n",
    "        states = []\n",
    "        for _ in range(steps_per_epoch):\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "            action_probs, value = network(state)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action.item())\n",
    "            log_prob = dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            masks.append(1-done)\n",
    "            actions.append(action)\n",
    "            states.append(state)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                state, info = env.reset()\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        _, next_value = network(next_state)\n",
    "        returns = compute_gae(next_value, rewards, masks, values, gamma, tau)\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        advantage = returns - values\n",
    "\n",
    "        # Update policy and value networks\n",
    "        _, new_values = network(states)\n",
    "        dist = Categorical(network.actor(states))\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
    "                            1.0 + clip_param) * advantage\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - new_values).pow(2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {policy_loss.item()}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train('CartPole-v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
