{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import HandcraftedAgent\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "\n",
    "capacity = {\"episodic\": 32, \"episodic_agent\": 32, \"semantic\": 32, \"short\": 1}\n",
    "\n",
    "config = {\n",
    "    \"question_prob\": 1.0,\n",
    "    \"seed\": 0,\n",
    "    \"terminates_at\": 99,\n",
    "    \"randomize_observations\": True,\n",
    "    \"room_size\": \"m\",\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for mm_policy in [\"random\"]:\n",
    "    for qa_policy in [\"episodic_semantic\"]:\n",
    "        for explore_policy in [\"avoid_walls\"]:\n",
    "            for pretrain_semantic in [False]:\n",
    "                key = (\n",
    "                    mm_policy,\n",
    "                    qa_policy,\n",
    "                    explore_policy,\n",
    "                    pretrain_semantic,\n",
    "                )\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                print(key)\n",
    "\n",
    "                for seed in tqdm([0, 1, 2, 3, 4]):\n",
    "                    config[\"seed\"] = seed\n",
    "\n",
    "                    agent = HandcraftedAgent(\n",
    "                        env_str=\"room_env:RoomEnv-v2\",\n",
    "                        env_config=config,\n",
    "                        mm_policy=mm_policy,\n",
    "                        qa_policy=qa_policy,\n",
    "                        explore_policy=explore_policy,\n",
    "                        num_samples_for_results=10,\n",
    "                        capacity=capacity,\n",
    "                        pretrain_semantic=pretrain_semantic,\n",
    "                    )\n",
    "                    agent.test()\n",
    "                    agent.remove_results_from_disk()\n",
    "                    to_append = (np.mean(agent.scores), np.std(agent.scores))\n",
    "                    # print(to_append)\n",
    "                    results[key].append(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{'memory mgmt':<20}{'qa':<20}{'explore':<20}{'pretrain_semantic':<20}{'mean':<10}{'std':<10}\"\n",
    ")\n",
    "print(f\"-\" * 110)\n",
    "for key, val in results.items():\n",
    "    print(\n",
    "        f\"{str(key[0]):<20}{str(key[1]):<20}{str(key[2]):<20}{str(key[3]):<20}{round(np.mean([v[0] for v in val]), 2):<10}{round(np.std([v[0] for v in val]), 2):<10}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.dqn import DQNMMAgent\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "while True:\n",
    "    batch_size = random.choice([128, 256, 512, 1024])\n",
    "    warm_start = random.choice(\n",
    "        [batch_size, 2 * batch_size, 4 * batch_size, 8 * batch_size]\n",
    "    )\n",
    "    replay_buffer_size = random.choice(\n",
    "        [warm_start, 2 * warm_start, 4 * warm_start, 8 * warm_start]\n",
    "    )\n",
    "    config = {\n",
    "        \"question_prob\": 1.0,\n",
    "        \"terminates_at\": 99,\n",
    "        \"randomize_observations\": random.choice([True, False]),\n",
    "        \"room_size\": \"s\",\n",
    "    }\n",
    "    all_params = {\n",
    "        \"env_str\": \"room_env:RoomEnv-v2\",\n",
    "        \"max_epsilon\": 1.0,\n",
    "        \"min_epsilon\": 0.1,\n",
    "        \"epsilon_decay_until\": 100 * 10,\n",
    "        \"gamma\": random.random(),\n",
    "        \"capacity\": {\n",
    "            \"episodic\": 16,\n",
    "            \"episodic_agent\": 16,\n",
    "            \"semantic\": 16,\n",
    "            \"short\": 1,\n",
    "        },\n",
    "        \"nn_params\": {\n",
    "            \"hidden_size\": 64,\n",
    "            \"num_layers\": 2,\n",
    "            \"embedding_dim\": 32,\n",
    "            \"v1_params\": None,\n",
    "            \"v2_params\": {},\n",
    "            \"memory_of_interest\": [\n",
    "                \"episodic\",\n",
    "                \"semantic\",\n",
    "                \"short\",\n",
    "            ],\n",
    "        },\n",
    "        \"num_iterations\": 100 * 10,\n",
    "        \"replay_buffer_size\": replay_buffer_size,\n",
    "        \"warm_start\": warm_start,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"target_update_rate\": 10,\n",
    "        \"pretrain_semantic\": False,\n",
    "        \"run_test\": True,\n",
    "        \"num_samples_for_results\": 10,\n",
    "        \"train_seed\": 5,\n",
    "        \"plotting_interval\": 10,\n",
    "        \"device\": \"cpu\",\n",
    "        \"test_seed\": 0,\n",
    "        \"qa_policy\": \"episodic_semantic\",\n",
    "        \"explore_policy\": \"avoid_walls\",\n",
    "        \"env_config\": config,\n",
    "        \"ddqn\": True,\n",
    "        \"dueling_dqn\": True,\n",
    "        \"split_reward_training\": random.choice([True, False]),\n",
    "    }\n",
    "\n",
    "    agent = DQNMMAgent(**all_params)\n",
    "    agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
