{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episodic_only\n",
    "- mean 44.56\n",
    "- std 1.85\n",
    "\n",
    "semantic_only\n",
    "- mean 55.72\n",
    "- std 1.61\n",
    "\n",
    "random\n",
    "- mean 37.64 \n",
    "- std 2.16\n",
    "\n",
    "dqn, pretrained, no ddqn, no dueling (w/ warm_start)\n",
    "- mean 108.2\n",
    "- std 5.01\n",
    "\n",
    "dqn, no ddqn, no dueling (w/ warm_start)\n",
    "- mean 91.28\n",
    "- std 8.43\n",
    "\n",
    "The code with AAAI paper (not_pretrained, pretrained):\n",
    "\n",
    "- DQN:            110.7, 89.3\n",
    "- DDQN:           108.2, 90.2\n",
    "- Dueling DQN:    102.7, 81.4\n",
    "- Dueling DDQN:   109.3, 89.0\n",
    "\n",
    "After writing tests (pre_trained)\n",
    "- mean: 103.84\n",
    "- std: 5.34\n",
    "\n",
    "After writing tests (not_pre_trained)\n",
    "- mean: 83.96\n",
    "- std: 4.21\n",
    "\n",
    "Alright I don't know why I can't reproduce the original results. The performance is always about 7 points lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explicit_memory.utils import read_yaml\n",
    "from glob import glob\n",
    "\n",
    "test_scores = {\"pre_trained\": [], \"not_pre_trained\": []}\n",
    "for foo in glob(\"./training_results/after-writing-tests/pre-trained/*/results.yaml\"):\n",
    "    results = read_yaml(foo)\n",
    "    test_scores[\"pre_trained\"].append(results[\"test_score\"])\n",
    "\n",
    "for foo in glob(\n",
    "    \"./training_results/after-writing-tests/not-pre-trained/*/results.yaml\"\n",
    "):\n",
    "    results = read_yaml(foo)\n",
    "    test_scores[\"not_pre_trained\"].append(results[\"test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre_trained': [{'mean': 109.8, 'std': 5.69},\n",
       "  {'mean': 96.0, 'std': 5.66},\n",
       "  {'mean': 109.8, 'std': 4.94},\n",
       "  {'mean': 100.8, 'std': 7.0},\n",
       "  {'mean': 102.8, 'std': 7.81}],\n",
       " 'not_pre_trained': [{'mean': 89.4, 'std': 7.9},\n",
       "  {'mean': 83.8, 'std': 10.49},\n",
       "  {'mean': 87.8, 'std': 5.9},\n",
       "  {'mean': 78.2, 'std': 10.86},\n",
       "  {'mean': 80.6, 'std': 5.94}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103.84, 5.3447544377641885)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([foo[\"mean\"] for foo in test_scores[\"pre_trained\"]]), np.std(\n",
    "    [foo[\"mean\"] for foo in test_scores[\"pre_trained\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83.96, 4.215020759142238)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([foo[\"mean\"] for foo in test_scores[\"not_pre_trained\"]]), np.std(\n",
    "    [foo[\"mean\"] for foo in test_scores[\"not_pre_trained\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "from agent import DQNAgent\n",
    "\n",
    "for pretrain_semantic in [False, True]:\n",
    "    for test_seed in [0, 1, 2, 3, 4]:\n",
    "        all_params = {\n",
    "            \"env_str\": \"room_env:RoomEnv-v1\",\n",
    "            \"max_epsilon\": 1.0,\n",
    "            \"min_epsilon\": 0.1,\n",
    "            \"epsilon_decay_until\": 128 * 16,\n",
    "            \"gamma\": 0.65,\n",
    "            \"capacity\": {\"episodic\": 16, \"semantic\": 16, \"short\": 1},\n",
    "            \"nn_params\": {\n",
    "                \"hidden_size\": 64,\n",
    "                \"num_layers\": 2,\n",
    "                \"embedding_dim\": 32,\n",
    "                \"v1_params\": {\n",
    "                    \"include_human\": \"sum\",\n",
    "                    \"human_embedding_on_object_location\": False,\n",
    "                },\n",
    "                \"v2_params\": None,\n",
    "            },\n",
    "            \"num_iterations\": 128 * 16,\n",
    "            \"replay_buffer_size\": 1024 * 128,\n",
    "            \"warm_start\": 1024 * 128,\n",
    "            \"batch_size\": 1024,\n",
    "            \"target_update_rate\": 10,\n",
    "            \"pretrain_semantic\": pretrain_semantic,\n",
    "            \"run_test\": True,\n",
    "            \"num_samples_for_results\": 10,\n",
    "            \"train_seed\": test_seed + 5,\n",
    "            \"plotting_interval\": 10,\n",
    "            \"device\": \"cpu\",\n",
    "            \"test_seed\": test_seed,\n",
    "            \"ddqn\": False,\n",
    "            \"dueling_dqn\": False,\n",
    "        }\n",
    "\n",
    "        # all_params = {\n",
    "        #     \"env_str\": \"room_env:RoomEnv-v1\",\n",
    "        #     \"max_epsilon\": 1.0,\n",
    "        #     \"min_epsilon\": 0.1,\n",
    "        #     \"epsilon_decay_until\": 128 * 16,\n",
    "        #     \"gamma\": 0.65,\n",
    "        #     \"capacity\": {\"episodic\": 4, \"semantic\": 4, \"short\": 1},\n",
    "        #     \"nn_params\": {\n",
    "        #         \"hidden_size\": 8,\n",
    "        #         \"num_layers\": 2,\n",
    "        #         \"embedding_dim\": 4,\n",
    "        #         \"v1_params\": {\n",
    "        #             \"include_human\": \"sum\",\n",
    "        #             \"human_embedding_on_object_location\": False,\n",
    "        #         },\n",
    "        #         \"v2_params\": None,\n",
    "        #     },\n",
    "        #     \"num_iterations\": 128 * 16,\n",
    "        #     \"replay_buffer_size\": 16,\n",
    "        #     \"warm_start\": 16,\n",
    "        #     \"batch_size\": 4,\n",
    "        #     \"target_update_rate\": 10,\n",
    "        #     \"pretrain_semantic\": pretrain_semantic,\n",
    "        #     \"run_test\": True,\n",
    "        #     \"num_samples_for_results\": 3,\n",
    "        #     \"train_seed\": test_seed + 5,\n",
    "        #     \"plotting_interval\": 10,\n",
    "        #     \"device\": \"cpu\",\n",
    "        #     \"test_seed\": test_seed,\n",
    "        #     \"ddqn\": True,\n",
    "        #     \"dueling_dqn\": True,\n",
    "        # }\n",
    "\n",
    "        agent = DQNAgent(**all_params)\n",
    "        agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explicit_memory.utils import write_yaml\n",
    "\n",
    "write_yaml(all_params, \"train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from agent import HandcraftedAgent\n",
    "\n",
    "for policy in tqdm([\"random\", \"episodic_only\", \"semantic_only\"]):\n",
    "    for test_seed in [0, 1, 2, 3, 4]:\n",
    "        all_params = {\n",
    "            \"env_str\": \"room_env:RoomEnv-v1\",\n",
    "            \"policy\": policy,\n",
    "            \"num_samples_for_results\": 10,\n",
    "            \"seed\": test_seed,\n",
    "        }\n",
    "        if policy == \"random\":\n",
    "            all_params[\"capacity\"] = {\"episodic\": 16, \"semantic\": 16, \"short\": 1}\n",
    "        elif policy == \"episodic_only\":\n",
    "            all_params[\"capacity\"] = {\"episodic\": 32, \"semantic\": 0, \"short\": 1}\n",
    "        else:\n",
    "            all_params[\"capacity\"] = {\"episodic\": 0, \"semantic\": 32, \"short\": 1}\n",
    "\n",
    "        all_params[\"seed\"] = test_seed\n",
    "        agent = HandcraftedAgent(**all_params)\n",
    "        agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "results = glob(\"./training_results/refactoring/episodic_only_agent/*/results.yaml\")\n",
    "scores = []\n",
    "for file in results:\n",
    "    with open(file, \"r\") as f:\n",
    "        results = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    scores.append(results[\"test_score\"][\"mean\"])\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "results = glob(\"./training_results/refactoring/semantic_only_agent/*/results.yaml\")\n",
    "scores = []\n",
    "for file in results:\n",
    "    with open(file, \"r\") as f:\n",
    "        results = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    scores.append(results[\"test_score\"][\"mean\"])\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "results = glob(\"./training_results/refactoring/random_agent/*/results.yaml\")\n",
    "scores = []\n",
    "for file in results:\n",
    "    with open(file, \"r\") as f:\n",
    "        results = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    scores.append(results[\"test_score\"][\"mean\"])\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7c14ce45c674ffbe7e3a8bc18299264a1035542c780d18c0e8f0c585e044f28"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dev-python3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
